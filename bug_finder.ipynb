{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985464cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "166783f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "import urllib.parse\n",
    "\n",
    "import configparser\n",
    "# import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "from typing import Dict\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ce53569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1266011a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e8ce6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_path = './config.ini'\n",
    "\n",
    "if not Path(config_file_path).exists():\n",
    "    logger.warning(f\"Config file {config_file_path} not found!\")\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(config_file_path)\n",
    "\n",
    "if 'DATABASE' not in config:\n",
    "    raise ValueError(\"DATABASE section not found in config\")\n",
    "\n",
    "db_config = {\n",
    "    'host': config['DATABASE']['host'],\n",
    "    'port': int(config['DATABASE']['port']),\n",
    "    'username': config['DATABASE']['username'],\n",
    "    'password': config['DATABASE']['password'],\n",
    "    'database': config['DATABASE']['database'],\n",
    "    'query_request': config['DATABASE']['query1'],\n",
    "    'query_assets': config['DATABASE']['query2'],\n",
    "    'query_request_with_activities': config['DATABASE']['query3'],\n",
    "    'schema': config['DATABASE']['schema']\n",
    "}\n",
    "\n",
    "db_host = db_config.get('host')\n",
    "db_port = db_config.get('port')\n",
    "db_username = db_config.get('username')\n",
    "db_password = db_config.get('password')\n",
    "db_database = db_config.get('database')\n",
    "db_query1 = db_config.get('query_request')\n",
    "db_query2 = db_config.get('query_assets')\n",
    "db_query3 = db_config.get('query_request_with_activities')\n",
    "db_schema = db_config.get('schema')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d561af84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def database_connector( db_type, host, port, database, username, password, **kwargs):\n",
    "    \n",
    "        encoded_password = urllib.parse.quote_plus(password)\n",
    "        connection_strings = {\n",
    "            'postgresql': f\"postgresql://{username}:{encoded_password}@{host}:{port}/{database}\",\n",
    "        }\n",
    "        return connection_strings[db_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06fe5a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_string = database_connector(\n",
    "            db_type='postgresql',\n",
    "            host=db_host,\n",
    "            port=db_port,\n",
    "            database=db_database,\n",
    "            username=db_username,\n",
    "            password=db_password,\n",
    "            schema=db_schema\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c1d2c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_sync_date_time = '2025-11-01 07:00:00.000'\n",
    "# '2025-11-01 07:00:02.929'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9db9077e",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_time = pd.to_datetime(last_sync_date_time) + datetime.timedelta(days=1)\n",
    "upper_bound = pd.to_datetime(diff_time, format='mixed').strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56a08ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_bound = '2025-11-02 08:00:00.000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e846b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0edd0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: Downloaded 1 rows from 'v_requests', 508863 rows from 'v_assets', and 1 rows from 'v_request_with_activities'.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "        engine = create_engine(conn_string)\n",
    "        \n",
    "        if not db_query1:\n",
    "            logger.warning(\"Query for v_request is missing!\")\n",
    "            \n",
    "        else:\n",
    "            db_query1 = db_query1.replace(';',f'\\nWHERE \"requestCreatedDate\" >=\\'{last_sync_date_time}\\' and \"requestCreatedDate\" <= \\'{upper_bound}\\';')\n",
    "    \n",
    "        if not db_query2:\n",
    "            logger.warning(\"Query for v_assets is missing!\")\n",
    "           \n",
    "        # else:\n",
    "        #     self.db_query2 = self.db_query2.replace(';',f'\\nWHERE \"requestCreatedDate\" >=\\'{self.last_sync_date_time}\\';')\n",
    "        \n",
    "        if not db_query3:\n",
    "            logger.warning(\"Query for v_request_with_activities is missing!\")\n",
    "            \n",
    "        else:\n",
    "            db_query3 = db_query3.replace(';',f'\\nWHERE \"requestCreatedDate\" >=\\'{last_sync_date_time}\\' and \"requestCreatedDate\" <= \\'{upper_bound}\\';')\n",
    "\n",
    "        # print(self.db_query1)\n",
    "        # print(self.db_query2)\n",
    "        # print(self.db_query3)\n",
    "\n",
    "        df_request = pd.read_sql(db_query1, engine)\n",
    "        df_assets = pd.read_sql(db_query2, engine)\n",
    "        # df_assets = pd.read_csv('./fetched_data/v_assets.csv')\n",
    "        df_request_with_activities = pd.read_sql(db_query3, engine)\n",
    "        \n",
    "        logger.info(f\" Downloaded {len(df_request)} rows from 'v_requests', {len(df_assets)} rows from 'v_assets', and {len(df_request_with_activities)} rows from 'v_request_with_activities'.\")\n",
    "        \n",
    "        # Saving the data (Don't save when pushing incrementally!!)\n",
    "        # # self.df_request.to_csv(f\"{target_dir_path}/v_requests.csv\",index=False)\n",
    "        # # self.df_assets.to_csv(f\"{target_dir_path}/v_assets.csv\",index=False)\n",
    "        # # self.df_request_with_activities.to_csv(f\"{target_dir_path}/v_requests_with_activities.csv\",index=False)\n",
    "        \n",
    "        # logger.info(f\"Data exported to {target_dir_path}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    logger.warning(f\"Error connecting to database: {e}\")\n",
    "    \n",
    "\n",
    "finally:\n",
    "    if 'engine' in locals():\n",
    "            engine.dispose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36463790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>requestId</th>\n",
       "      <th>requestAlternateId</th>\n",
       "      <th>workType</th>\n",
       "      <th>requestDescription</th>\n",
       "      <th>requestCreatedDate</th>\n",
       "      <th>requestTargetCompletionDate</th>\n",
       "      <th>requestCompletionDate</th>\n",
       "      <th>serviceClassificationId</th>\n",
       "      <th>serviceClassificationAlternateId</th>\n",
       "      <th>serviceClassificationPath</th>\n",
       "      <th>...</th>\n",
       "      <th>locationAlternateId</th>\n",
       "      <th>locationPath</th>\n",
       "      <th>assetId</th>\n",
       "      <th>assetAlternateId</th>\n",
       "      <th>assetDescription</th>\n",
       "      <th>completionNotes</th>\n",
       "      <th>customer</th>\n",
       "      <th>country</th>\n",
       "      <th>isSelfAssign</th>\n",
       "      <th>priorityCode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b71e6dbd-4a3f-41f2-ad82-1174c7f8c5b7</td>\n",
       "      <td>RRBC100309</td>\n",
       "      <td>Proactive</td>\n",
       "      <td>TESTING</td>\n",
       "      <td>2025-11-01 16:12:15.788</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>6d4a97a7-f1e4-4c1c-b37a-46a9c8cf35ee</td>\n",
       "      <td>SCRBC100235</td>\n",
       "      <td>Cleaning Services | Carpet Cleaning | Carpet -...</td>\n",
       "      <td>...</td>\n",
       "      <td>LUS508405</td>\n",
       "      <td>US, AZ, Peoria / RBC-AZ001DR, Peoria-Sun City ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Royal Bank of Canada</td>\n",
       "      <td>US</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              requestId requestAlternateId   workType  \\\n",
       "0  b71e6dbd-4a3f-41f2-ad82-1174c7f8c5b7         RRBC100309  Proactive   \n",
       "\n",
       "  requestDescription      requestCreatedDate requestTargetCompletionDate  \\\n",
       "0            TESTING 2025-11-01 16:12:15.788                        None   \n",
       "\n",
       "  requestCompletionDate               serviceClassificationId  \\\n",
       "0                  None  6d4a97a7-f1e4-4c1c-b37a-46a9c8cf35ee   \n",
       "\n",
       "  serviceClassificationAlternateId  \\\n",
       "0                      SCRBC100235   \n",
       "\n",
       "                           serviceClassificationPath  ... locationAlternateId  \\\n",
       "0  Cleaning Services | Carpet Cleaning | Carpet -...  ...           LUS508405   \n",
       "\n",
       "                                        locationPath assetId assetAlternateId  \\\n",
       "0  US, AZ, Peoria / RBC-AZ001DR, Peoria-Sun City ...    None             None   \n",
       "\n",
       "  assetDescription completionNotes              customer country isSelfAssign  \\\n",
       "0             None            None  Royal Bank of Canada      US        False   \n",
       "\n",
       "   priorityCode  \n",
       "0          None  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b9bb5b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>requestId</th>\n",
       "      <th>requestAlternateId</th>\n",
       "      <th>workType</th>\n",
       "      <th>requestDescription</th>\n",
       "      <th>requestCreatedDate</th>\n",
       "      <th>requestTargetCompletionDate</th>\n",
       "      <th>requestCompletionDate</th>\n",
       "      <th>activityId</th>\n",
       "      <th>activityAlternateId</th>\n",
       "      <th>activityDescription</th>\n",
       "      <th>activityStartDate</th>\n",
       "      <th>activityCompletionDate</th>\n",
       "      <th>completionNotes</th>\n",
       "      <th>providertype</th>\n",
       "      <th>customer</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b71e6dbd-4a3f-41f2-ad82-1174c7f8c5b7</td>\n",
       "      <td>RRBC100309</td>\n",
       "      <td>Proactive</td>\n",
       "      <td>TESTING</td>\n",
       "      <td>2025-11-01 16:12:15.788</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Royal Bank of Canada</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              requestId requestAlternateId   workType  \\\n",
       "0  b71e6dbd-4a3f-41f2-ad82-1174c7f8c5b7         RRBC100309  Proactive   \n",
       "\n",
       "  requestDescription      requestCreatedDate requestTargetCompletionDate  \\\n",
       "0            TESTING 2025-11-01 16:12:15.788                        None   \n",
       "\n",
       "  requestCompletionDate activityId activityAlternateId activityDescription  \\\n",
       "0                  None       None                None                None   \n",
       "\n",
       "  activityStartDate activityCompletionDate completionNotes providertype  \\\n",
       "0              None                   None            None         None   \n",
       "\n",
       "               customer country  \n",
       "0  Royal Bank of Canada      US  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_request_with_activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d61a60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cc6dfcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_hvac_df_ = pd.read_csv('./data/hvac_assets/IFM_Assets_RuleBasedEngineResults(IFM_Assets_RuleBasedEngineResul).csv')\n",
    "suggested_asset_df = pd.read_csv('./data/asset_suggest_data/asset_suggest_model.csv')\n",
    "vendor_data = pd.read_csv('./data/asset_vendor/request_act_vendor.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddc11f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Node and Property data prepared!\n"
     ]
    }
   ],
   "source": [
    "# def data_preprocessor(self):\n",
    "try:\n",
    "    # Activity:\n",
    "    activity_df = df_request_with_activities[df_request_with_activities['activityAlternateId'].notna()][['providertype','activityAlternateId','activityDescription']]\n",
    "    activity_df.drop_duplicates(inplace = True)\n",
    "\n",
    "    # Asset:\n",
    "    requests_subset = df_request[['requestId','assetAlternateId', 'requestAlternateId']]\n",
    "    requests_subset = requests_subset[requests_subset.assetAlternateId.notna()]\n",
    "\n",
    "    v_assets = df_assets[['assetId','Asset Alt Id', 'Asset Description', 'manufacturer', 'model', 'serialNumber']]\n",
    "    v_assets = v_assets.merge(requests_subset, left_on = 'Asset Alt Id', right_on = 'assetAlternateId', how= 'left')\n",
    "    v_assets = v_assets[v_assets['requestAlternateId'].notna()] # keeping only those asset records which are associated to the presently fetched serviceRequests\n",
    "\n",
    "    is_hvac_df = is_hvac_df_.copy()\n",
    "    is_hvac_df['is_HVAC'] = True\n",
    "    is_hvac_df.drop(columns=['Asset Description'], inplace = True)\n",
    "    v_assets_with_hvac = v_assets.merge(is_hvac_df, on='Asset Alt Id', how='left')\n",
    "\n",
    "    if not v_assets_with_hvac.empty:\n",
    "        v_assets_with_hvac.loc[v_assets_with_hvac['is_HVAC'] == True, 'asset_type'] = 'HVAC'\n",
    "    else:\n",
    "        v_assets_with_hvac['asset_type'] = None\n",
    "\n",
    "    final_assets_df = v_assets_with_hvac[['assetId', 'Asset Description', 'Asset Alt Id', 'manufacturer', 'model',\n",
    "                                            'serialNumber', 'is_HVAC', 'asset_type', 'requestId','assetAlternateId', 'requestAlternateId']]\n",
    "    final_assets_df.loc[:, 'is_HVAC'] = final_assets_df['is_HVAC'].fillna(False)\n",
    "\n",
    "    suggested_asset_df = suggested_asset_df.copy()\n",
    "    suggested_asset_df.rename(columns={'asset_id': 'suggested_asset'}, inplace=True)\n",
    "    suggested_asset_df_subset = suggested_asset_df[['request_id', 'suggested_asset']]\n",
    "\n",
    "    final_assets_df = final_assets_df.merge(suggested_asset_df_subset, left_on = 'requestId', right_on = 'request_id', how = 'left')\n",
    "    final_assets_df = final_assets_df[['assetId', 'Asset Description', 'Asset Alt Id', 'manufacturer', 'model',\n",
    "                                        'serialNumber', 'is_HVAC', 'asset_type', 'suggested_asset','requestAlternateId']]\n",
    "\n",
    "    vendor_data = vendor_data.copy()\n",
    "    vendor_data = vendor_data[['requestAlternateId','vendorName', 'vendorAddress1', 'vendorCity',\n",
    "                        'vendorRegion', 'vendorCountry', 'vendorPostalCode']]\n",
    "    assets_with_vendors = final_assets_df.merge(vendor_data, on = 'requestAlternateId', how = 'left')\n",
    "    assets_df = assets_with_vendors[['Asset Description', 'Asset Alt Id', 'manufacturer', 'model','serialNumber', \n",
    "                                                'is_HVAC', 'asset_type', 'suggested_asset','vendorName', 'vendorAddress1', 'vendorCity',\n",
    "                                                'vendorRegion', 'vendorCountry', 'vendorPostalCode']]\n",
    "\n",
    "    # Country:\n",
    "    country_df = df_request[['country']].drop_duplicates()\n",
    "\n",
    "    # Customer:\n",
    "    customer_df = df_request[['customer']].drop_duplicates()\n",
    "\n",
    "    # Location:\n",
    "    location_df = df_request[['locationAlternateId', 'locationPath']].drop_duplicates()\n",
    "\n",
    "    # Service Requests:\n",
    "    temp_ser_req = df_request[['isSelfAssign', 'priorityCode', \n",
    "                'requestCreatedDate', 'requestDescription', 'requestAlternateId', 'completionNotes', \n",
    "                'requestTargetCompletionDate', 'serviceClassificationAlternateId', 'serviceClassificationPath',  \n",
    "                'requestCompletionDate', 'workType']]\n",
    "\n",
    "    def to_local_datetime(date_col):\n",
    "\n",
    "        if date_col is None:\n",
    "            return None\n",
    "        \n",
    "        if date_col.isna().all():\n",
    "            return date_col\n",
    "        \n",
    "        dt_series = pd.to_datetime(date_col, format='mixed')\n",
    "        formatted = dt_series.dt.strftime('%Y-%m-%d %H:%M:%S.%f').str[:-3]\n",
    "        \n",
    "        return formatted.str.replace(' ', 'T')\n",
    "\n",
    "\n",
    "    def process_service_requests(df_service_request):\n",
    "            \n",
    "            date_cols = ['requestCreatedDate', 'requestTargetCompletionDate', 'requestCompletionDate']\n",
    "            \n",
    "            for col in date_cols:\n",
    "                if col in df_service_request.columns:\n",
    "                    df_service_request.loc[:, col] = to_local_datetime(df_service_request[col])\n",
    "            \n",
    "            df_service_request['createdYear'] = pd.to_datetime(df_service_request['requestCreatedDate']).dt.year\n",
    "            df_service_request['createdMonth'] = pd.to_datetime(df_service_request['requestCreatedDate']).dt.month\n",
    "                \n",
    "            df_service_request['isCompleted'] = df_service_request['requestCompletionDate'].notna()\n",
    "            \n",
    "            conditions = [\n",
    "                df_service_request['requestCompletionDate'].isna(),\n",
    "                df_service_request['requestTargetCompletionDate'].isna(),\n",
    "                df_service_request['requestCompletionDate'] <= df_service_request['requestTargetCompletionDate'],\n",
    "                df_service_request['requestCompletionDate'] > df_service_request['requestTargetCompletionDate']\n",
    "            ]\n",
    "            \n",
    "            choices = ['Open', 'Open', 'Met', 'Miss']\n",
    "            \n",
    "            df_service_request['sla'] = np.select(conditions, choices, default='Unknown')\n",
    "            \n",
    "            return df_service_request\n",
    "\n",
    "\n",
    "    service_req_df = process_service_requests(temp_ser_req)\n",
    "\n",
    "    activity_df = activity_df.copy()\n",
    "    assets_df = assets_df.copy()\n",
    "    country_df = country_df.copy()\n",
    "    customer_df = customer_df.copy()\n",
    "    location_df = location_df.copy()\n",
    "    service_req_df = service_req_df.copy()\n",
    "    logger.info(\"Node and Property data prepared!\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Error preprocessing data: {e}\")\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "80adc1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # renaming the features:\n",
    "    activity_df.rename(columns={'activityAlternateId': 'activityId', 'providertype':'providerType'}, inplace=True)\n",
    "\n",
    "    assets_df.rename(columns={'Asset Alt Id': 'assetId', 'Asset Description':'assetDescription', 'vendorAddress1':'vendorAddress'}, inplace=True)\n",
    "\n",
    "    location_df.rename(columns={'locationAlternateId': 'locationId'}, inplace=True)\n",
    "\n",
    "    service_req_df.rename(columns={'requestAlternateId': 'requestId', 'serviceClassificationAlternateId': 'serviceClassificationId'}, inplace=True)\n",
    "\n",
    "    # logger.info(f\"Data for migration to Neo4J is saved on path: {neo4j_dir_path} and ready to be imported!\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Error Renaming Features: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3a1ceea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Relationships created!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "\n",
    "    LOCATED_AT = df_request[['assetAlternateId','locationAlternateId']].dropna().drop_duplicates()\n",
    "    LOCATED_AT.rename(columns={'assetAlternateId': 'assetId', 'locationAlternateId': 'locationId'}, inplace=True)\n",
    "    # self.LOCATED_AT.to_csv(f\"{neo4j_relationship_dir_path}/LOCATED_AT.csv\",index=False)\n",
    "\n",
    "    AT_LOCATION = df_request[['requestAlternateId','locationAlternateId']].dropna().drop_duplicates()\n",
    "    AT_LOCATION.rename(columns={'requestAlternateId': 'requestId', 'locationAlternateId': 'locationId'}, inplace=True)\n",
    "    # self.AT_LOCATION.to_csv(f\"{neo4j_relationship_dir_path}/AT_LOCATION.csv\",index=False)\n",
    "\n",
    "    HAS_ACTIVITY = df_request_with_activities[['activityAlternateId', 'requestAlternateId']].dropna().drop_duplicates()\n",
    "    HAS_ACTIVITY.rename(columns={'requestAlternateId': 'requestId', 'activityAlternateId': 'activityId'}, inplace=True)\n",
    "    # self.HAS_ACTIVITY.to_csv(f\"{neo4j_relationship_dir_path}/HAS_ACTIVITY.csv\",index=False)\n",
    "\n",
    "    FOR_ASSET = df_request[['requestAlternateId','assetAlternateId']].dropna().drop_duplicates()\n",
    "    FOR_ASSET.rename(columns={'requestAlternateId': 'requestId', 'assetAlternateId': 'assetId'}, inplace=True)\n",
    "    # self.FOR_ASSET.to_csv(f\"{neo4j_relationship_dir_path}/FOR_ASSET.csv\",index=False)\n",
    "\n",
    "    OPERATES_IN = df_request[['customer','country']].dropna().drop_duplicates()\n",
    "    # self.OPERATES_IN.to_csv(f\"{neo4j_relationship_dir_path}/OPERATES_IN.csv\",index=False)\n",
    "\n",
    "    RESIDES_AT = df_request[['customer','locationAlternateId']].dropna().drop_duplicates()\n",
    "    RESIDES_AT.rename(columns={'locationAlternateId': 'locationId'}, inplace=True)\n",
    "    # self.RESIDES_AT.to_csv(f\"{neo4j_relationship_dir_path}/RESIDES_AT.csv\",index=False)\n",
    "\n",
    "    OWNS = df_request[['customer','assetAlternateId']].dropna().drop_duplicates()\n",
    "    OWNS.rename(columns={'assetAlternateId': 'assetId'}, inplace=True)\n",
    "    # self.OWNS.to_csv(f\"{neo4j_relationship_dir_path}/OWNS.csv\",index=False)\n",
    "\n",
    "    CREATES = df_request[['customer','requestAlternateId']].dropna().drop_duplicates()\n",
    "    CREATES.rename(columns={'requestAlternateId': 'requestId'}, inplace=True)\n",
    "    # self.CREATES.to_csv(f\"{neo4j_relationship_dir_path}/CREATES.csv\",index=False)\n",
    "\n",
    "    IN = df_request[['country','locationAlternateId']].dropna().drop_duplicates()\n",
    "    IN.rename(columns={'locationAlternateId': 'locationId'}, inplace=True)\n",
    "    # self.IN.to_csv(f\"{neo4j_relationship_dir_path}/IN.csv\",index=False)\n",
    "\n",
    "    logger.info(f\"Relationships created!\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Error while creating and saving relationships: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "78f3a32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Path(config_file_path).exists():\n",
    "    logger.warning(f\"Config file {config_file_path} not found!\")\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(config_file_path)\n",
    "\n",
    "if 'Neo4j' not in config:\n",
    "    raise ValueError(\"Neo4j section not found in config\")\n",
    "\n",
    "nj_config = {\n",
    "    'url': config['Neo4j']['url'],\n",
    "    'username': config['Neo4j']['username'],\n",
    "    'password': config['Neo4j']['password']\n",
    "}\n",
    "\n",
    "nj_url = nj_config.get('url')\n",
    "nj_username = nj_config.get('username')\n",
    "nj_password = nj_config.get('password')\n",
    "\n",
    "driver = GraphDatabase.driver(nj_url, auth=(nj_username, nj_password))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ffcbffb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nodes_from_csv( csv, node_label: str, id_property: str, batch_size: int = 1000):\n",
    "        \"\"\"Load nodes from CSV file in batches.\"\"\"\n",
    "        df = csv\n",
    "        df = df.where(pd.notnull(df), None)  # Replace NaN with None\n",
    "        \n",
    "        total_rows = len(df)\n",
    "        # logger.info(f\"Loading {total_rows} {node_label} nodes from {csv_path}\")\n",
    "        \n",
    "        with driver.session() as session:\n",
    "            for i in range(0, total_rows, batch_size):\n",
    "                batch = df.iloc[i:i+batch_size]\n",
    "                records = batch.to_dict('records')\n",
    "                \n",
    "                # Build Cypher query dynamically\n",
    "                query = f\"\"\"\n",
    "                UNWIND $records AS record\n",
    "                MERGE (n:{node_label} {{{id_property}: record.{id_property}}})\n",
    "                SET n += record\n",
    "                \"\"\"\n",
    "                session.run(query, records=records)\n",
    "                logger.info(f\"Loaded batch {i//batch_size + 1}/{(total_rows-1)//batch_size + 1} for {node_label}\")\n",
    "        \n",
    "        logger.info(f\"Completed loading {node_label} nodes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ea2db1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Completed loading Activity nodes\n",
      "INFO:__main__:Completed loading Asset nodes\n",
      "INFO:__main__:Loaded batch 1/1 for Country\n",
      "INFO:__main__:Completed loading Country nodes\n",
      "INFO:__main__:Loaded batch 1/1 for Customer\n",
      "INFO:__main__:Completed loading Customer nodes\n",
      "INFO:__main__:Loaded batch 1/1 for Location\n",
      "INFO:__main__:Completed loading Location nodes\n",
      "INFO:__main__:Loaded batch 1/1 for ServiceRequest\n",
      "INFO:__main__:Completed loading ServiceRequest nodes\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "\n",
    "    # # Create constraints and indexes              # not required when hydrating data to a preexisting graph\n",
    "    # self.create_constraints()\n",
    "    # self.create_indexes()\n",
    "    \n",
    "    # Load nodes\n",
    "    \n",
    "    load_nodes_from_csv(\n",
    "        activity_df, \n",
    "        \"Activity\", \n",
    "        \"activityId\"\n",
    "    )\n",
    "    \n",
    "    load_nodes_from_csv(\n",
    "        assets_df, \n",
    "        \"Asset\", \n",
    "        \"assetId\"\n",
    "    )\n",
    "    \n",
    "    load_nodes_from_csv(\n",
    "        country_df, \n",
    "        \"Country\", \n",
    "        \"country\"\n",
    "    )\n",
    "    \n",
    "    load_nodes_from_csv(\n",
    "        customer_df, \n",
    "        \"Customer\", \n",
    "        \"customer\"\n",
    "    )\n",
    "    \n",
    "    load_nodes_from_csv(\n",
    "        location_df, \n",
    "        \"Location\", \n",
    "        \"locationId\"\n",
    "    )\n",
    "    \n",
    "    load_nodes_from_csv(\n",
    "        service_req_df, \n",
    "        \"ServiceRequest\", \n",
    "        \"requestId\"\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Error creating Nodes in Neo4j: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5d0dc55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_relationships_from_csv(csv, rel_config: Dict, batch_size: int = 1000):\n",
    "    \"\"\"\n",
    "    Load relationships from CSV file.\n",
    "    \n",
    "    rel_config example:\n",
    "    {\n",
    "        'rel_type': 'LOCATED_AT',\n",
    "        'from_label': 'Asset',\n",
    "        'from_id_col': 'assetId',\n",
    "        'from_id_prop': 'assetId',\n",
    "        'to_label': 'Location',\n",
    "        'to_id_col': 'locationId',\n",
    "        'to_id_prop': 'locationId',\n",
    "        'properties': []  # Optional: list of relationship properties\n",
    "    }\n",
    "    \"\"\"\n",
    "    df = csv\n",
    "    df = df.where(pd.notnull(df), None)\n",
    "    \n",
    "    total_rows = len(df)\n",
    "    # logger.info(f\"Loading {total_rows} {rel_config['rel_type']} relationships from {csv_path}\")\n",
    "    \n",
    "    with driver.session() as session:\n",
    "        for i in range(0, total_rows, batch_size):\n",
    "            batch = df.iloc[i:i+batch_size]\n",
    "            records = batch.to_dict('records')\n",
    "            \n",
    "            # Build relationship properties string if any\n",
    "            rel_props = \"\"\n",
    "            if rel_config.get('properties'):\n",
    "                props_str = \", \".join([f\"{p}: record.{p}\" for p in rel_config['properties']])\n",
    "                rel_props = f\" {{{props_str}}}\"\n",
    "            \n",
    "            query = f\"\"\"\n",
    "            UNWIND $records AS record\n",
    "            MATCH (from:{rel_config['from_label']} {{{rel_config['from_id_prop']}: record.{rel_config['from_id_col']}}})\n",
    "            MATCH (to:{rel_config['to_label']} {{{rel_config['to_id_prop']}: record.{rel_config['to_id_col']}}})\n",
    "            MERGE (from)-[r:{rel_config['rel_type']}]->(to)\n",
    "            \"\"\"\n",
    "            \n",
    "            if rel_props:\n",
    "                query += f\"\\nSET r += {{{', '.join([f'{p}: record.{p}' for p in rel_config['properties']])}}}\"\n",
    "            \n",
    "            session.run(query, records=records)\n",
    "            logger.info(f\"Loaded batch {i//batch_size + 1}/{(total_rows-1)//batch_size + 1} for {rel_config['rel_type']}\")\n",
    "    \n",
    "    logger.info(f\"Completed loading {rel_config['rel_type']} relationships\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d1bbdba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Completed loading LOCATED_AT relationships\n",
      "INFO:__main__:Loaded batch 1/1 for AT_LOCATION\n",
      "INFO:__main__:Completed loading AT_LOCATION relationships\n",
      "INFO:__main__:Completed loading HAS_ACTIVITY relationships\n",
      "INFO:__main__:Completed loading FOR_ASSET relationships\n",
      "INFO:__main__:Loaded batch 1/1 for OPERATES_IN\n",
      "INFO:__main__:Completed loading OPERATES_IN relationships\n",
      "INFO:__main__:Loaded batch 1/1 for RESIDES_AT\n",
      "INFO:__main__:Completed loading RESIDES_AT relationships\n",
      "INFO:__main__:Completed loading OWNS relationships\n",
      "INFO:__main__:Loaded batch 1/1 for CREATES\n",
      "INFO:__main__:Completed loading CREATES relationships\n",
      "INFO:__main__:Loaded batch 1/1 for IN\n",
      "INFO:__main__:Completed loading IN relationships\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Load relationships\n",
    "    \n",
    "    # Asset -> Location\n",
    "    load_relationships_from_csv(\n",
    "        LOCATED_AT,\n",
    "        {\n",
    "            'rel_type': 'LOCATED_AT',\n",
    "            'from_label': 'Asset',\n",
    "            'from_id_col': 'assetId',\n",
    "            'from_id_prop': 'assetId',\n",
    "            'to_label': 'Location',\n",
    "            'to_id_col': 'locationId',\n",
    "            'to_id_prop': 'locationId'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # ServiceRequest -> Location\n",
    "    load_relationships_from_csv(\n",
    "        AT_LOCATION,\n",
    "        {\n",
    "            'rel_type': 'AT_LOCATION',\n",
    "            'from_label': 'ServiceRequest',\n",
    "            'from_id_col': 'requestId',\n",
    "            'from_id_prop': 'requestId',\n",
    "            'to_label': 'Location',\n",
    "            'to_id_col': 'locationId',\n",
    "            'to_id_prop': 'locationId'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # ServiceRequest -> Activity\n",
    "    load_relationships_from_csv(\n",
    "        HAS_ACTIVITY,\n",
    "        {\n",
    "            'rel_type': 'HAS_ACTIVITY',\n",
    "            'from_label': 'ServiceRequest',\n",
    "            'from_id_col': 'requestId',\n",
    "            'from_id_prop': 'requestId',\n",
    "            'to_label': 'Activity',\n",
    "            'to_id_col': 'activityId',\n",
    "            'to_id_prop': 'activityId'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # ServiceRequest -> Asset\n",
    "    load_relationships_from_csv(\n",
    "        FOR_ASSET,\n",
    "        {\n",
    "            'rel_type': 'FOR_ASSET',\n",
    "            'from_label': 'ServiceRequest',\n",
    "            'from_id_col': 'requestId',\n",
    "            'from_id_prop': 'requestId',\n",
    "            'to_label': 'Asset',\n",
    "            'to_id_col': 'assetId',\n",
    "            'to_id_prop': 'assetId'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Customer -> Country\n",
    "    load_relationships_from_csv(\n",
    "        OPERATES_IN,\n",
    "        {\n",
    "            'rel_type': 'OPERATES_IN',\n",
    "            'from_label': 'Customer',\n",
    "            'from_id_col': 'customer',\n",
    "            'from_id_prop': 'customer',\n",
    "            'to_label': 'Country',\n",
    "            'to_id_col': 'country',\n",
    "            'to_id_prop': 'country'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Customer -> Location\n",
    "    load_relationships_from_csv(\n",
    "        RESIDES_AT,\n",
    "        {\n",
    "            'rel_type': 'RESIDES_AT',\n",
    "            'from_label': 'Customer',\n",
    "            'from_id_col': 'customer',\n",
    "            'from_id_prop': 'customer',\n",
    "            'to_label': 'Location',\n",
    "            'to_id_col': 'locationId',\n",
    "            'to_id_prop': 'locationId'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Customer -> Asset\n",
    "    load_relationships_from_csv(\n",
    "        OWNS,\n",
    "        {\n",
    "            'rel_type': 'OWNS',\n",
    "            'from_label': 'Customer',\n",
    "            'from_id_col': 'customer',\n",
    "            'from_id_prop': 'customer',\n",
    "            'to_label': 'Asset',\n",
    "            'to_id_col': 'assetId',\n",
    "            'to_id_prop': 'assetId'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Customer -> ServiceRequest\n",
    "    load_relationships_from_csv(\n",
    "        CREATES,\n",
    "        {\n",
    "            'rel_type': 'CREATES',\n",
    "            'from_label': 'Customer',\n",
    "            'from_id_col': 'customer',\n",
    "            'from_id_prop': 'customer',\n",
    "            'to_label': 'ServiceRequest',\n",
    "            'to_id_col': 'requestId',\n",
    "            'to_id_prop': 'requestId'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Location -> Country\n",
    "    load_relationships_from_csv(\n",
    "        IN,\n",
    "        {\n",
    "            'rel_type': 'IN',\n",
    "            'from_label': 'Location',\n",
    "            'from_id_col': 'locationId',\n",
    "            'from_id_prop': 'locationId',\n",
    "            'to_label': 'Country',\n",
    "            'to_id_col': 'country',\n",
    "            'to_id_prop': 'country'\n",
    "        }\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Error creating Relationships for nodes on Neo4j: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd752d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'\\x9c6\\xaf\\xf4\\xd7T]M,\\xc5\\x8etZ\\xb7\\x01\\xa1\\x0c\\x1f\\x00\\x01|\\x00\\x00\\x00\\x01\\x00\\x02\\x00\\x03\\x00\\x04\\x00\\x05\\x00\\x06\\x00\\x07\\x00\\x08\\x00\\t\\x00\\n\\x00\\x0b\\x00\\x0c\\x00\\r\\x00\\x0e\\x00\\x0f\\x00\\x10\\x00\\x11\\x00\\x12\\x00\\x13\\x00\\x14\\x00\\x15\\x00\\x16\\x00\\x17\\x00\\x18\\x00\\x19\\x00\\x1a\\x00\\x1b\\x00/\\x000\\x001\\x002\\x003\\x004\\x005\\x006\\x007\\x008\\x009\\x00:\\x00;\\x00<\\x00=\\x00>\\x00?\\x00@\\x00A\\x00B\\x00C\\x00D\\x00E\\x00F\\x00g\\x00h\\x00i\\x00j\\x00k\\x00l\\x00m\\x00\\x84\\x00\\x85\\x00\\x86\\x00\\x87\\x00\\x88\\x00\\x89\\x00\\x96\\x00\\x97\\x00\\x98\\x00\\x99\\x00\\x9a\\x00\\x9b\\x00\\x9c\\x00\\x9d\\x00\\x9e\\x00\\x9f\\x00\\xa0\\x00\\xa1\\x00\\xa2\\x00\\xa3\\x00\\xa4\\x00\\xa5\\x00\\xa6\\x00\\xa7\\x00\\xba\\x00\\xbb\\x00\\xbc\\x00\\xbd\\x00\\xbe\\x00\\xbf\\x00\\xc0\\x00\\xc1\\x00\\xc2\\x00']\n",
      "Bad pipe message: %s [b\"44\\xb2c\\xc9\\x1c]'U\\xe9\\xb9\\x06\\xc0\\xa7\\xe8\\x8do\\x8a\\x00\\x01|\\x00\\x00\\x00\\x01\\x00\\x02\\x00\\x03\\x00\\x04\\x00\\x05\\x00\\x06\\x00\\x07\\x00\\x08\\x00\\t\\x00\\n\\x00\\x0b\\x00\\x0c\\x00\\r\\x00\\x0e\\x00\\x0f\\x00\\x10\\x00\\x11\\x00\\x12\\x00\\x13\\x00\\x14\\x00\\x15\\x00\\x16\\x00\\x17\\x00\\x18\\x00\\x19\\x00\\x1a\\x00\\x1b\\x00/\\x000\\x001\\x002\\x003\\x004\\x005\\x006\\x007\\x008\\x009\\x00:\\x00;\\x00<\\x00=\\x00>\\x00?\\x00@\\x00A\\x00B\\x00C\\x00D\\x00E\\x00F\\x00g\\x00h\\x00i\\x00j\\x00k\\x00l\\x00m\\x00\\x84\\x00\\x85\\x00\\x86\\x00\\x87\\x00\\x88\\x00\\x89\\x00\\x96\\x00\\x97\\x00\\x98\\x00\\x99\\x00\\x9a\\x00\\x9b\\x00\\x9c\\x00\\x9d\\x00\\x9e\\x00\\x9f\\x00\\xa0\\x00\\xa1\\x00\\xa2\\x00\\xa3\\x00\\xa4\\x00\\xa5\\x00\\xa6\\x00\\xa7\\x00\\xba\\x00\\xbb\\x00\\xbc\\x00\\xbd\\x00\\xbe\\x00\\xbf\\x00\\xc0\\x00\\xc1\\x00\\xc2\\x00\\xc3\\x00\\xc4\\x00\\xc5\\x13\\x01\\x13\\x02\\x13\\x03\\x13\\x04\\x13\\x05\\xc0\\x01\\xc0\\x02\\xc0\\x03\\xc0\\x04\\xc0\\x05\\xc0\\x06\\xc0\\x07\\xc0\", b'\\t\\xc0\\n\\xc0\\x0b\\xc0\\x0c']\n",
      "Bad pipe message: %s [b'+\\xae\\xe2', b'\\xfb\\xb6l\\x90\\xad.\\xfc\\x8b\\xb8D\\x02\\xbb+\\x00\\x01|\\x00\\x00\\x00\\x01\\x00\\x02\\x00\\x03\\x00\\x04\\x00\\x05\\x00\\x06\\x00\\x07\\x00\\x08\\x00\\t\\x00\\n\\x00\\x0b\\x00\\x0c\\x00\\r\\x00\\x0e\\x00\\x0f\\x00\\x10\\x00\\x11\\x00\\x12\\x00\\x13\\x00\\x14\\x00\\x15\\x00\\x16\\x00\\x17\\x00\\x18\\x00\\x19\\x00\\x1a\\x00\\x1b\\x00/\\x000\\x001\\x002\\x003\\x004\\x005\\x006\\x007\\x008\\x009\\x00:\\x00;\\x00<\\x00=\\x00>\\x00?\\x00@\\x00A\\x00B\\x00C\\x00D\\x00E\\x00F\\x00g\\x00h\\x00i\\x00j\\x00k\\x00l\\x00m\\x00\\x84\\x00\\x85\\x00\\x86\\x00\\x87\\x00\\x88\\x00\\x89\\x00\\x96\\x00\\x97\\x00\\x98\\x00\\x99\\x00\\x9a\\x00\\x9b\\x00\\x9c\\x00\\x9d\\x00\\x9e\\x00\\x9f\\x00\\xa0\\x00\\xa1\\x00\\xa2\\x00\\xa3\\x00\\xa4\\x00\\xa5\\x00\\xa6\\x00\\xa7\\x00\\xba\\x00\\xbb\\x00\\xbc\\x00\\xbd\\x00\\xbe\\x00\\xbf\\x00\\xc0\\x00\\xc1\\x00\\xc2\\x00\\xc3\\x00\\xc4\\x00\\xc5\\x13\\x01\\x13\\x02\\x13\\x03\\x13\\x04\\x13\\x05\\xc0\\x01\\xc0\\x02\\xc0\\x03\\xc0\\x04\\xc0\\x05\\xc0\\x06']\n",
      "Bad pipe message: %s [b'\\xb1A\\x9d\\x92]\\xd2\\xb5\\xf8l\\xfd\\x08\\xcbO\\x1a\\xcf']\n",
      "Bad pipe message: %s [b\"\\xc4\\x00\\xc5\\x13\\x01\\x13\\x02\\x13\\x03\\x13\\x04\\x13\\x05\\xc0\\x01\\xc0\\x02\\xc0\\x03\\xc0\\x04\\xc0\\x05\\xc0\\x06\\xc0\\x07\\xc0\\x08\\xc0\\t\\xc0\\n\\xc0\\x0b\\xc0\\x0c\\xc0\\r\\xc0\\x0e\\xc0\\x0f\\xc0\\x10\\xc0\\x11\\xc0\\x12\\xc0\\x13\\xc0\\x14\\xc0\\x15\\xc0\\x16\\xc0\\x17\\xc0\\x18\\xc0\\x19\\xc0#\\xc0$\\xc0%\\xc0&\\xc0'\\xc0(\\xc0)\\xc0*\\xc0+\\xc0,\\xc0-\\xc0.\\xc0/\\xc00\\xc01\\xc02\\xc0r\\xc0s\\xc0t\\xc0u\\xc0v\\xc0w\\xc0x\\xc0y\\xc0z\\xc0{\\xc0|\\xc0}\\xc0~\\xc0\\x7f\\xc0\\x80\\xc0\\x81\\xc0\\x82\\xc0\\x83\\xc0\\x84\\xc0\\x85\\xc0\\x86\\xc0\\x87\\xc0\\x88\\xc0\\x89\\xc0\\x8a\\xc0\\x8b\\xc0\\x8c\\xc0\\x8d\\xc0\\x8e\\xc0\\x8f\\xc0\\x90\\xc0\\x91\\xc0\\x92\\xc0\\x93\\xc0\\x94\\xc0\\x95\\xc0\\x96\\xc0\\x97\\xc0\\x98\\xc0\\x99\\xc0\\x9a\\xc0\\x9b\\xcc\\xa8\\xcc\\xa9\\xcc\\xaa\\xcc\\xab\\xcc\\xac\\xcc\\xad\\xcc\\xae\\x02\"]\n",
      "Bad pipe message: %s [b\"\\xc0\\x08\\xc0\\t\\xc0\\n\\xc0\\x0b\\xc0\\x0c\\xc0\\r\\xc0\\x0e\\xc0\\x0f\\xc0\\x10\\xc0\\x11\\xc0\\x12\\xc0\\x13\\xc0\\x14\\xc0\\x15\\xc0\\x16\\xc0\\x17\\xc0\\x18\\xc0\\x19\\xc0#\\xc0$\\xc0%\\xc0&\\xc0'\\xc0(\\xc0)\\xc0*\\xc0+\\xc0,\\xc0-\\xc0.\\xc0/\\xc00\\xc01\\xc02\\xc0r\\xc0s\\xc0t\\xc0u\\xc0v\\xc0w\\xc0x\\xc0y\\xc0z\\xc0{\\xc0|\\xc0}\\xc0~\\xc0\\x7f\\xc0\\x80\\xc0\\x81\\xc0\\x82\\xc0\\x83\\xc0\\x84\\xc0\\x85\\xc0\\x86\\xc0\\x87\\xc0\\x88\\xc0\\x89\\xc0\\x8a\\xc0\\x8b\\xc0\\x8c\\xc0\\x8d\\xc0\\x8e\\xc0\\x8f\\xc0\\x90\\xc0\\x91\\xc0\\x92\\xc0\\x93\\xc0\\x94\\xc0\\x95\\xc0\\x96\\xc0\\x97\\xc0\\x98\\xc0\\x99\\xc0\\x9a\\xc0\\x9b\\xcc\\xa8\\xcc\\xa9\\xcc\\xaa\\xcc\\xab\\xcc\\xac\\xcc\\xad\\xcc\\xae\\x02\\x00\\x01\\x00\\x16\\x00\\n\\x00\\n\\x00\\x08\\x00\\x17\\x00\\x19\\x00\\x18\\x00\\x16\\x00\\x0b\\x00\\x04\\x03\\x00\", b'']\n",
      "ERROR:tornado.general:Uncaught exception in ZMQStream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/SChauhan17/Desktop/projects/data_ingestion_pipeline/data_injestion/lib/python3.13/site-packages/zmq/eventloop/zmqstream.py\", line 565, in _log_error\n",
      "    f.result()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/Users/SChauhan17/Desktop/projects/data_ingestion_pipeline/data_injestion/lib/python3.13/site-packages/ipykernel/kernelbase.py\", line 663, in shell_channel_thread_main\n",
      "    _, msg2 = self.session.feed_identities(msg, copy=False)\n",
      "              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/SChauhan17/Desktop/projects/data_ingestion_pipeline/data_injestion/lib/python3.13/site-packages/jupyter_client/session.py\", line 994, in feed_identities\n",
      "    raise ValueError(msg)\n",
      "ValueError: DELIM not in msg_list\n",
      "ERROR:tornado.general:Uncaught exception in ZMQStream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/SChauhan17/Desktop/projects/data_ingestion_pipeline/data_injestion/lib/python3.13/site-packages/zmq/eventloop/zmqstream.py\", line 565, in _log_error\n",
      "    f.result()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/Users/SChauhan17/Desktop/projects/data_ingestion_pipeline/data_injestion/lib/python3.13/site-packages/ipykernel/kernelbase.py\", line 663, in shell_channel_thread_main\n",
      "    _, msg2 = self.session.feed_identities(msg, copy=False)\n",
      "              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/SChauhan17/Desktop/projects/data_ingestion_pipeline/data_injestion/lib/python3.13/site-packages/jupyter_client/session.py\", line 994, in feed_identities\n",
      "    raise ValueError(msg)\n",
      "ValueError: DELIM not in msg_list\n",
      "ERROR:tornado.general:Uncaught exception in ZMQStream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/SChauhan17/Desktop/projects/data_ingestion_pipeline/data_injestion/lib/python3.13/site-packages/zmq/eventloop/zmqstream.py\", line 565, in _log_error\n",
      "    f.result()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/Users/SChauhan17/Desktop/projects/data_ingestion_pipeline/data_injestion/lib/python3.13/site-packages/ipykernel/kernelbase.py\", line 663, in shell_channel_thread_main\n",
      "    _, msg2 = self.session.feed_identities(msg, copy=False)\n",
      "              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/SChauhan17/Desktop/projects/data_ingestion_pipeline/data_injestion/lib/python3.13/site-packages/jupyter_client/session.py\", line 994, in feed_identities\n",
      "    raise ValueError(msg)\n",
      "ValueError: DELIM not in msg_list\n",
      "ERROR:tornado.general:Uncaught exception in ZMQStream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/SChauhan17/Desktop/projects/data_ingestion_pipeline/data_injestion/lib/python3.13/site-packages/zmq/eventloop/zmqstream.py\", line 565, in _log_error\n",
      "    f.result()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/Users/SChauhan17/Desktop/projects/data_ingestion_pipeline/data_injestion/lib/python3.13/site-packages/ipykernel/kernelbase.py\", line 339, in dispatch_control\n",
      "    await self.process_control(msg)\n",
      "  File \"/Users/SChauhan17/Desktop/projects/data_ingestion_pipeline/data_injestion/lib/python3.13/site-packages/ipykernel/kernelbase.py\", line 345, in process_control\n",
      "    idents, msg = self.session.feed_identities(msg, copy=False)\n",
      "                  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/SChauhan17/Desktop/projects/data_ingestion_pipeline/data_injestion/lib/python3.13/site-packages/jupyter_client/session.py\", line 994, in feed_identities\n",
      "    raise ValueError(msg)\n",
      "ValueError: DELIM not in msg_list\n",
      "ERROR:tornado.general:Uncaught exception in ZMQStream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/SChauhan17/Desktop/projects/data_ingestion_pipeline/data_injestion/lib/python3.13/site-packages/zmq/eventloop/zmqstream.py\", line 565, in _log_error\n",
      "    f.result()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/Users/SChauhan17/Desktop/projects/data_ingestion_pipeline/data_injestion/lib/python3.13/site-packages/ipykernel/kernelbase.py\", line 339, in dispatch_control\n",
      "    await self.process_control(msg)\n",
      "  File \"/Users/SChauhan17/Desktop/projects/data_ingestion_pipeline/data_injestion/lib/python3.13/site-packages/ipykernel/kernelbase.py\", line 345, in process_control\n",
      "    idents, msg = self.session.feed_identities(msg, copy=False)\n",
      "                  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/SChauhan17/Desktop/projects/data_ingestion_pipeline/data_injestion/lib/python3.13/site-packages/jupyter_client/session.py\", line 994, in feed_identities\n",
      "    raise ValueError(msg)\n",
      "ValueError: DELIM not in msg_list\n",
      "ERROR:tornado.general:Uncaught exception in ZMQStream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/SChauhan17/Desktop/projects/data_ingestion_pipeline/data_injestion/lib/python3.13/site-packages/zmq/eventloop/zmqstream.py\", line 565, in _log_error\n",
      "    f.result()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/Users/SChauhan17/Desktop/projects/data_ingestion_pipeline/data_injestion/lib/python3.13/site-packages/ipykernel/kernelbase.py\", line 339, in dispatch_control\n",
      "    await self.process_control(msg)\n",
      "  File \"/Users/SChauhan17/Desktop/projects/data_ingestion_pipeline/data_injestion/lib/python3.13/site-packages/ipykernel/kernelbase.py\", line 345, in process_control\n",
      "    idents, msg = self.session.feed_identities(msg, copy=False)\n",
      "                  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/SChauhan17/Desktop/projects/data_ingestion_pipeline/data_injestion/lib/python3.13/site-packages/jupyter_client/session.py\", line 994, in feed_identities\n",
      "    raise ValueError(msg)\n",
      "ValueError: DELIM not in msg_list\n",
      "ERROR:tornado.general:Uncaught exception in ZMQStream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/SChauhan17/Desktop/projects/data_ingestion_pipeline/data_injestion/lib/python3.13/site-packages/zmq/eventloop/zmqstream.py\", line 565, in _log_error\n",
      "    f.result()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/Users/SChauhan17/Desktop/projects/data_ingestion_pipeline/data_injestion/lib/python3.13/site-packages/ipykernel/kernelbase.py\", line 339, in dispatch_control\n",
      "    await self.process_control(msg)\n",
      "  File \"/Users/SChauhan17/Desktop/projects/data_ingestion_pipeline/data_injestion/lib/python3.13/site-packages/ipykernel/kernelbase.py\", line 345, in process_control\n",
      "    idents, msg = self.session.feed_identities(msg, copy=False)\n",
      "                  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/SChauhan17/Desktop/projects/data_ingestion_pipeline/data_injestion/lib/python3.13/site-packages/jupyter_client/session.py\", line 994, in feed_identities\n",
      "    raise ValueError(msg)\n",
      "ValueError: DELIM not in msg_list\n",
      "ERROR:tornado.general:Uncaught exception in ZMQStream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/SChauhan17/Desktop/projects/data_ingestion_pipeline/data_injestion/lib/python3.13/site-packages/zmq/eventloop/zmqstream.py\", line 565, in _log_error\n",
      "    f.result()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/Users/SChauhan17/Desktop/projects/data_ingestion_pipeline/data_injestion/lib/python3.13/site-packages/ipykernel/kernelbase.py\", line 339, in dispatch_control\n",
      "    await self.process_control(msg)\n",
      "  File \"/Users/SChauhan17/Desktop/projects/data_ingestion_pipeline/data_injestion/lib/python3.13/site-packages/ipykernel/kernelbase.py\", line 345, in process_control\n",
      "    idents, msg = self.session.feed_identities(msg, copy=False)\n",
      "                  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/SChauhan17/Desktop/projects/data_ingestion_pipeline/data_injestion/lib/python3.13/site-packages/jupyter_client/session.py\", line 994, in feed_identities\n",
      "    raise ValueError(msg)\n",
      "ValueError: DELIM not in msg_list\n",
      "ERROR:tornado.general:Uncaught exception in ZMQStream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/SChauhan17/Desktop/projects/data_ingestion_pipeline/data_injestion/lib/python3.13/site-packages/zmq/eventloop/zmqstream.py\", line 565, in _log_error\n",
      "    f.result()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/Users/SChauhan17/Desktop/projects/data_ingestion_pipeline/data_injestion/lib/python3.13/site-packages/ipykernel/kernelbase.py\", line 339, in dispatch_control\n",
      "    await self.process_control(msg)\n",
      "  File \"/Users/SChauhan17/Desktop/projects/data_ingestion_pipeline/data_injestion/lib/python3.13/site-packages/ipykernel/kernelbase.py\", line 345, in process_control\n",
      "    idents, msg = self.session.feed_identities(msg, copy=False)\n",
      "                  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/SChauhan17/Desktop/projects/data_ingestion_pipeline/data_injestion/lib/python3.13/site-packages/jupyter_client/session.py\", line 994, in feed_identities\n",
      "    raise ValueError(msg)\n",
      "ValueError: DELIM not in msg_list\n",
      "ERROR:tornado.general:Uncaught exception in ZMQStream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/SChauhan17/Desktop/projects/data_ingestion_pipeline/data_injestion/lib/python3.13/site-packages/zmq/eventloop/zmqstream.py\", line 565, in _log_error\n",
      "    f.result()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/Users/SChauhan17/Desktop/projects/data_ingestion_pipeline/data_injestion/lib/python3.13/site-packages/ipykernel/kernelbase.py\", line 339, in dispatch_control\n",
      "    await self.process_control(msg)\n",
      "  File \"/Users/SChauhan17/Desktop/projects/data_ingestion_pipeline/data_injestion/lib/python3.13/site-packages/ipykernel/kernelbase.py\", line 345, in process_control\n",
      "    idents, msg = self.session.feed_identities(msg, copy=False)\n",
      "                  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/SChauhan17/Desktop/projects/data_ingestion_pipeline/data_injestion/lib/python3.13/site-packages/jupyter_client/session.py\", line 994, in feed_identities\n",
      "    raise ValueError(msg)\n",
      "ValueError: DELIM not in msg_list\n"
     ]
    }
   ],
   "source": [
    "def close(self):\n",
    "    self.driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c830e13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18be6db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_injestion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
