{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4909ac11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "import urllib.parse\n",
    "\n",
    "import configparser\n",
    "# import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "789b76c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0121f0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderAndMigrator:\n",
    "\n",
    "    def __init__(self, config_file_path):\n",
    "\n",
    "        #-- LOADER --#\n",
    "        if not Path(config_file_path).exists():\n",
    "            logger.warning(f\"Config file {config_file_path} not found!\")\n",
    "        \n",
    "        config = configparser.ConfigParser()\n",
    "        config.read(config_file_path)\n",
    "\n",
    "        if 'DATABASE' not in config:\n",
    "            raise ValueError(\"DATABASE section not found in config\")\n",
    "        \n",
    "        db_config = {\n",
    "            'host': config['DATABASE']['host'],\n",
    "            'port': int(config['DATABASE']['port']),\n",
    "            'username': config['DATABASE']['username'],\n",
    "            'password': config['DATABASE']['password'],\n",
    "            'database': config['DATABASE']['database'],\n",
    "            'query_request': config['DATABASE']['query1'],\n",
    "            'query_assets': config['DATABASE']['query2'],\n",
    "            'query_request_with_activities': config['DATABASE']['query3'],\n",
    "            'schema': config['DATABASE']['schema']\n",
    "        }\n",
    "\n",
    "        self.db_host = db_config.get('host')\n",
    "        self.db_port = db_config.get('port')\n",
    "        self.db_username = db_config.get('username')\n",
    "        self.db_password = db_config.get('password')\n",
    "        self.db_database = db_config.get('database')\n",
    "        self.db_query1 = db_config.get('query_request')\n",
    "        self.db_query2 = db_config.get('query_assets')\n",
    "        self.db_query3 = db_config.get('query_request_with_activities')\n",
    "        self.db_schema = db_config.get('schema')\n",
    "        self.last_sync_date_time = None\n",
    "        self.sync_check()\n",
    "        self.executor()\n",
    "\n",
    "        #-- MIGRATION --#\n",
    "\n",
    "        if not Path(config_file_path).exists():\n",
    "            logger.warning(f\"Config file {config_file_path} not found!\")\n",
    "        \n",
    "        config = configparser.ConfigParser()\n",
    "        config.read(config_file_path)\n",
    "\n",
    "        if 'Neo4j' not in config:\n",
    "            raise ValueError(\"Neo4j section not found in config\")\n",
    "        \n",
    "        nj_config = {\n",
    "            'url': config['Neo4j']['url'],\n",
    "            'username': config['Neo4j']['username'],\n",
    "            'password': config['Neo4j']['password']\n",
    "        }\n",
    "\n",
    "        self.nj_url = nj_config.get('url')\n",
    "        self.nj_username = nj_config.get('username')\n",
    "        self.nj_password = nj_config.get('password')\n",
    "\n",
    "        self.driver = GraphDatabase.driver(self.nj_url, auth=(self.nj_username, self.nj_password))\n",
    "        \n",
    "    \n",
    "    def sync_check(self):\n",
    "\n",
    "        log_file_name = \"last_call_time.txt\"        # 2025-10-27 00:00:00.001\n",
    "        log_file_path = Path(log_file_name)\n",
    "\n",
    "        if log_file_path.is_file(): \n",
    "            with open(log_file_path, \"r\") as f:\n",
    "                last_call_str = f.read().strip()\n",
    "                if last_call_str:\n",
    "                    try:\n",
    "                        last_call_time = datetime.datetime.strptime(last_call_str, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "                        last_call_time = pd.to_datetime(last_call_time, format='mixed').strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]\n",
    "                        logger.info(f\"Data last pushed on: {last_call_time}\")\n",
    "                        self.last_sync_date_time = last_call_time\n",
    "                    except ValueError:\n",
    "                        logger.warning(\"Could not parse last call time from log file. It might be corrupted.\")\n",
    "\n",
    "    \n",
    "    def sync_time_updater(self):\n",
    "        log_file_name = \"last_call_time.txt\"\n",
    "        log_file_path = Path(log_file_name)\n",
    "\n",
    "        current_time = datetime.datetime.now()\n",
    "        current_time = pd.to_datetime(current_time, format='mixed').strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]\n",
    "\n",
    "        with open(log_file_path, \"w\") as f:\n",
    "            f.write(current_time)\n",
    "        \n",
    "\n",
    "    def executor(self):\n",
    "        self.conn_string = self.database_connector(\n",
    "            db_type='postgresql',\n",
    "            host=self.db_host,\n",
    "            port=self.db_port,\n",
    "            database=self.db_database,\n",
    "            username=self.db_username,\n",
    "            password=self.db_password,\n",
    "            schema=self.db_schema\n",
    "            )\n",
    "        \n",
    "        logger.info(\"=\" * 50)\n",
    "        logger.info(\"LOADING DATA\")\n",
    "        logger.info(\"=\" * 50)\n",
    "\n",
    "        self.load_and_save_data()\n",
    "        self.load_CSVs()\n",
    "        self.data_preprocessor()\n",
    "        self.save_neo4j_CSVs()\n",
    "        self.create_and_save_relationships()\n",
    "\n",
    "        logger.info(\"=\" * 50)\n",
    "        logger.info(\"DATA LOADING SUCCESSFUL!\")\n",
    "        logger.info(\"=\" * 50)\n",
    "\n",
    "        # self.sync_time_updater()\n",
    "\n",
    "    def database_connector(self, db_type, host, port, database, username, password, **kwargs):\n",
    "    \n",
    "        encoded_password = urllib.parse.quote_plus(password)\n",
    "        connection_strings = {\n",
    "            'postgresql': f\"postgresql://{username}:{encoded_password}@{host}:{port}/{database}\",\n",
    "        }\n",
    "        return connection_strings[db_type]\n",
    "    \n",
    "    def load_and_save_data(self):\n",
    "        \n",
    "        target_dir_path = './fetched_data'\n",
    "        Path(target_dir_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            engine = create_engine(self.conn_string)\n",
    "            \n",
    "            if not self.db_query1:\n",
    "                logger.warning(\"Query for v_request is missing!\")\n",
    "                return None \n",
    "            else:\n",
    "                self.db_query1 = self.db_query1.replace(';',f'\\nWHERE \"requestCreatedDate\" >=\\'{self.last_sync_date_time}\\';')\n",
    "        \n",
    "            if not self.db_query2:\n",
    "                logger.warning(\"Query for v_assets is missing!\")\n",
    "                return None\n",
    "            # else:\n",
    "            #     self.db_query2 = self.db_query2.replace(';',f'\\nWHERE \"requestCreatedDate\" >=\\'{self.last_sync_date_time}\\';')\n",
    "            \n",
    "            if not self.db_query3:\n",
    "                logger.warning(\"Query for v_request_with_activities is missing!\")\n",
    "                return None\n",
    "            else:\n",
    "                self.db_query3 = self.db_query3.replace(';',f'\\nWHERE \"requestCreatedDate\" >=\\'{self.last_sync_date_time}\\';')\n",
    "\n",
    "            # print(self.db_query1)\n",
    "            # print(self.db_query2)\n",
    "            # print(self.db_query3)\n",
    "\n",
    "            self.df_request = pd.read_sql(self.db_query1, engine)\n",
    "            # self.df_assets = pd.read_sql(self.db_query2, engine)\n",
    "            self.df_assets = pd.read_csv('./fetched_data/v_assets.csv')\n",
    "            self.df_request_with_activities = pd.read_sql(self.db_query3, engine)\n",
    "            \n",
    "            logger.info(f\" Downloaded {len(self.df_request)} rows from 'v_requests', {len(self.df_assets)} rows from 'v_assets', and {len(self.df_request_with_activities)} rows from 'v_request_with_activities'.\")\n",
    "            \n",
    "            # Saving the data (Don't save when pushing incrementally!!)\n",
    "            # # self.df_request.to_csv(f\"{target_dir_path}/v_requests.csv\",index=False)\n",
    "            # # self.df_assets.to_csv(f\"{target_dir_path}/v_assets.csv\",index=False)\n",
    "            # # self.df_request_with_activities.to_csv(f\"{target_dir_path}/v_requests_with_activities.csv\",index=False)\n",
    "            \n",
    "            # logger.info(f\"Data exported to {target_dir_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error connecting to database: {e}\")\n",
    "            return None\n",
    "        \n",
    "        finally:\n",
    "            if 'engine' in locals():\n",
    "                engine.dispose()\n",
    "    \n",
    "    def load_CSVs(self):\n",
    "        self.is_hvac_df = pd.read_csv('./data/hvac_assets/IFM_Assets_RuleBasedEngineResults(IFM_Assets_RuleBasedEngineResul).csv')\n",
    "        self.suggested_asset_df = pd.read_csv('./data/asset_suggest_data/asset_suggest_model.csv')\n",
    "        self.vendor_data = pd.read_csv('./data/asset_vendor/request_act_vendor.csv')\n",
    "\n",
    "        logger.info(\"Helper CSV files loaded successfully.\")\n",
    "\n",
    "\n",
    "    def data_preprocessor(self):\n",
    "\n",
    "        # Activity:\n",
    "        activity_df = self.df_request_with_activities[self.df_request_with_activities['activityAlternateId'].notna()][['providertype','activityAlternateId','activityDescription']]\n",
    "        activity_df.drop_duplicates(inplace = True)\n",
    "\n",
    "        # Asset:\n",
    "        requests_subset = self.df_request[['requestId','assetAlternateId', 'requestAlternateId']]\n",
    "        requests_subset = requests_subset[requests_subset.assetAlternateId.notna()]\n",
    "\n",
    "        v_assets = self.df_assets[['assetId','Asset Alt Id', 'Asset Description', 'manufacturer', 'model', 'serialNumber']]\n",
    "        v_assets = v_assets.merge(requests_subset, left_on = 'Asset Alt Id', right_on = 'assetAlternateId', how= 'left')\n",
    "        v_assets = v_assets[v_assets['requestAlternateId'].notna()] # keeping only those asset records which are associated to the presently fetched serviceRequests\n",
    "\n",
    "        is_hvac_df = self.is_hvac_df.copy()\n",
    "        is_hvac_df['is_HVAC'] = True\n",
    "        is_hvac_df.drop(columns=['Asset Description'], inplace = True)\n",
    "        v_assets_with_hvac = v_assets.merge(is_hvac_df, on='Asset Alt Id', how='left')\n",
    "        v_assets_with_hvac.loc[v_assets_with_hvac['is_HVAC'] == True, 'asset_type'] = 'HVAC'\n",
    "        final_assets_df = v_assets_with_hvac[['assetId', 'Asset Description', 'Asset Alt Id', 'manufacturer', 'model',\n",
    "                                              'serialNumber', 'is_HVAC', 'asset_type', 'requestId','assetAlternateId', 'requestAlternateId']]\n",
    "        final_assets_df.loc[:, 'is_HVAC'] = final_assets_df['is_HVAC'].fillna(False)\n",
    "\n",
    "        suggested_asset_df = self.suggested_asset_df.copy()\n",
    "        suggested_asset_df.rename(columns={'asset_id': 'suggested_asset'}, inplace=True)\n",
    "        suggested_asset_df_subset = suggested_asset_df[['request_id', 'suggested_asset']]\n",
    "        final_assets_df = final_assets_df.merge(suggested_asset_df_subset, left_on = 'requestId', right_on = 'request_id', how = 'left')\n",
    "        final_assets_df = final_assets_df[['assetId', 'Asset Description', 'Asset Alt Id', 'manufacturer', 'model',\n",
    "                                           'serialNumber', 'is_HVAC', 'asset_type', 'suggested_asset','requestAlternateId']]\n",
    "\n",
    "        vendor_data = self.vendor_data.copy()\n",
    "        vendor_data = vendor_data[['requestAlternateId','vendorName', 'vendorAddress1', 'vendorCity',\n",
    "                           'vendorRegion', 'vendorCountry', 'vendorPostalCode']]\n",
    "        assets_with_vendors = final_assets_df.merge(vendor_data, on = 'requestAlternateId', how = 'left')\n",
    "        assets_df = assets_with_vendors[['Asset Description', 'Asset Alt Id', 'manufacturer', 'model','serialNumber', \n",
    "                                                   'is_HVAC', 'asset_type', 'suggested_asset','vendorName', 'vendorAddress1', 'vendorCity',\n",
    "                                                   'vendorRegion', 'vendorCountry', 'vendorPostalCode']]\n",
    "\n",
    "        # Country:\n",
    "        country_df = self.df_request[['country']].drop_duplicates()\n",
    "\n",
    "        # Customer:\n",
    "        customer_df = self.df_request[['customer']].drop_duplicates()\n",
    "\n",
    "        # Location:\n",
    "        location_df = self.df_request[['locationAlternateId', 'locationPath']].drop_duplicates()\n",
    "\n",
    "        # Service Requests:\n",
    "        temp_ser_req = self.df_request[['isSelfAssign', 'priorityCode', \n",
    "                  'requestCreatedDate', 'requestDescription', 'requestAlternateId', 'completionNotes', \n",
    "                  'requestTargetCompletionDate', 'serviceClassificationAlternateId', 'serviceClassificationPath',  \n",
    "                  'requestCompletionDate', 'workType']]\n",
    "        \n",
    "        def to_local_datetime(date_col):\n",
    "    \n",
    "            if date_col is None:\n",
    "                return None\n",
    "            \n",
    "            if date_col.isna().all():\n",
    "                return date_col\n",
    "            \n",
    "            dt_series = pd.to_datetime(date_col, format='mixed')\n",
    "            formatted = dt_series.dt.strftime('%Y-%m-%d %H:%M:%S.%f').str[:-3]\n",
    "            \n",
    "            return formatted.str.replace(' ', 'T')\n",
    "\n",
    "\n",
    "        def process_service_requests(df_service_request):\n",
    "                \n",
    "                date_cols = ['requestCreatedDate', 'requestTargetCompletionDate', 'requestCompletionDate']\n",
    "                \n",
    "                for col in date_cols:\n",
    "                    if col in df_service_request.columns:\n",
    "                        df_service_request.loc[:, col] = to_local_datetime(df_service_request[col])\n",
    "                \n",
    "                \n",
    "                df_service_request['createdYear'] = pd.to_datetime(df_service_request['requestCreatedDate']).dt.year\n",
    "                df_service_request['createdMonth'] = pd.to_datetime(df_service_request['requestCreatedDate']).dt.month\n",
    "                \n",
    "                \n",
    "                df_service_request['isCompleted'] = df_service_request['requestCompletionDate'].notna()\n",
    "                \n",
    "                \n",
    "                conditions = [\n",
    "                    df_service_request['requestCompletionDate'].isna(),\n",
    "                    df_service_request['requestTargetCompletionDate'].isna(),\n",
    "                    df_service_request['requestCompletionDate'] <= df_service_request['requestTargetCompletionDate'],\n",
    "                    df_service_request['requestCompletionDate'] > df_service_request['requestTargetCompletionDate']\n",
    "                ]\n",
    "                \n",
    "                choices = ['Open', 'Open', 'Met', 'Miss']\n",
    "                \n",
    "                df_service_request['sla'] = np.select(conditions, choices, default='Unknown')\n",
    "                \n",
    "                return df_service_request\n",
    "\n",
    "        \n",
    "        service_req_df = process_service_requests(temp_ser_req)\n",
    "\n",
    "        self.activity_df = activity_df.copy()\n",
    "        self.assets_df = assets_df.copy()\n",
    "        self.country_df = country_df.copy()\n",
    "        self.customer_df = customer_df.copy()\n",
    "        self.location_df = location_df.copy()\n",
    "        self.service_req_df = service_req_df.copy()\n",
    "        logger.info(\"Node and Property data prepared!\")\n",
    "\n",
    "    def save_neo4j_CSVs(self):\n",
    "        neo4j_dir_path = './neo4j_data'\n",
    "        Path(neo4j_dir_path).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            # renaming the features:\n",
    "            self.activity_df.rename(columns={'activityAlternateId': 'activityId', 'providertype':'providerType'}, inplace=True)\n",
    "\n",
    "            self.assets_df.rename(columns={'Asset Alt Id': 'assetId', 'Asset Description':'assetDescription', 'vendorAddress1':'vendorAddress'}, inplace=True)\n",
    "\n",
    "            self.location_df.rename(columns={'locationAlternateId': 'locationId'}, inplace=True)\n",
    "\n",
    "            self.service_req_df.rename(columns={'requestAlternateId': 'requestId', 'serviceClassificationAlternateId': 'serviceClassificationId'}, inplace=True)\n",
    "\n",
    "            # # saving the data\n",
    "            # self.activity_df.to_csv(f\"{neo4j_dir_path}/activities.csv\",index=False)\n",
    "            # self.assets_df.to_csv(f\"{neo4j_dir_path}/assets.csv\",index=False)\n",
    "            # self.country_df.to_csv(f\"{neo4j_dir_path}/countries.csv\",index=False)\n",
    "            # self.customer_df.to_csv(f\"{neo4j_dir_path}/customers.csv\",index=False)\n",
    "            # self.location_df.to_csv(f\"{neo4j_dir_path}/location.csv\",index=False)\n",
    "            # self.service_req_df.to_csv(f\"{neo4j_dir_path}/service_requests.csv\",index=False)\n",
    "\n",
    "            # logger.info(f\"Data for migration to Neo4J is saved on path: {neo4j_dir_path} and ready to be imported!\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error Renaming Features: {e}\")\n",
    "    \n",
    "    def create_and_save_relationships(self):\n",
    "\n",
    "        neo4j_relationship_dir_path = './neo4j_relationships'\n",
    "        Path(neo4j_relationship_dir_path).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "\n",
    "            self.LOCATED_AT = self.df_request[['assetAlternateId','locationAlternateId']].dropna().drop_duplicates()\n",
    "            self.LOCATED_AT.rename(columns={'assetAlternateId': 'assetId', 'locationAlternateId': 'locationId'}, inplace=True)\n",
    "            # self.LOCATED_AT.to_csv(f\"{neo4j_relationship_dir_path}/LOCATED_AT.csv\",index=False)\n",
    "\n",
    "            self.AT_LOCATION = self.df_request[['requestAlternateId','locationAlternateId']].dropna().drop_duplicates()\n",
    "            self.AT_LOCATION.rename(columns={'requestAlternateId': 'requestId', 'locationAlternateId': 'locationId'}, inplace=True)\n",
    "            # self.AT_LOCATION.to_csv(f\"{neo4j_relationship_dir_path}/AT_LOCATION.csv\",index=False)\n",
    "\n",
    "            self.HAS_ACTIVITY = self.df_request_with_activities[['activityAlternateId', 'requestAlternateId']].dropna().drop_duplicates()\n",
    "            self.HAS_ACTIVITY.rename(columns={'requestAlternateId': 'requestId', 'activityAlternateId': 'activityId'}, inplace=True)\n",
    "            # self.HAS_ACTIVITY.to_csv(f\"{neo4j_relationship_dir_path}/HAS_ACTIVITY.csv\",index=False)\n",
    "\n",
    "            self.FOR_ASSET = self.df_request[['requestAlternateId','assetAlternateId']].dropna().drop_duplicates()\n",
    "            self.FOR_ASSET.rename(columns={'requestAlternateId': 'requestId', 'assetAlternateId': 'assetId'}, inplace=True)\n",
    "            # self.FOR_ASSET.to_csv(f\"{neo4j_relationship_dir_path}/FOR_ASSET.csv\",index=False)\n",
    "\n",
    "            self.OPERATES_IN = self.df_request[['customer','country']].dropna().drop_duplicates()\n",
    "            # self.OPERATES_IN.to_csv(f\"{neo4j_relationship_dir_path}/OPERATES_IN.csv\",index=False)\n",
    "\n",
    "            self.RESIDES_AT = self.df_request[['customer','locationAlternateId']].dropna().drop_duplicates()\n",
    "            self.RESIDES_AT.rename(columns={'locationAlternateId': 'locationId'}, inplace=True)\n",
    "            # self.RESIDES_AT.to_csv(f\"{neo4j_relationship_dir_path}/RESIDES_AT.csv\",index=False)\n",
    "\n",
    "            self.OWNS = self.df_request[['customer','assetAlternateId']].dropna().drop_duplicates()\n",
    "            self.OWNS.rename(columns={'assetAlternateId': 'assetId'}, inplace=True)\n",
    "            # self.OWNS.to_csv(f\"{neo4j_relationship_dir_path}/OWNS.csv\",index=False)\n",
    "\n",
    "            self.CREATES = self.df_request[['customer','requestAlternateId']].dropna().drop_duplicates()\n",
    "            self.CREATES.rename(columns={'requestAlternateId': 'requestId'}, inplace=True)\n",
    "            # self.CREATES.to_csv(f\"{neo4j_relationship_dir_path}/CREATES.csv\",index=False)\n",
    "\n",
    "            self.IN = self.df_request[['country','locationAlternateId']].dropna().drop_duplicates()\n",
    "            self.IN.rename(columns={'locationAlternateId': 'locationId'}, inplace=True)\n",
    "            # self.IN.to_csv(f\"{neo4j_relationship_dir_path}/IN.csv\",index=False)\n",
    "\n",
    "            logger.info(f\"Relationships created!\")\n",
    "\n",
    "        except Exception as e:\n",
    "\n",
    "            logger.warning(f\"Error while creating and saving relationships: {e}\")\n",
    "\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "    \n",
    "    def clear_database(self):\n",
    "\n",
    "        with self.driver.session() as session:\n",
    "\n",
    "            # Delete all nodes and relationships:\n",
    "            session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "            logger.info(\"All nodes and relationships deleted\")\n",
    "            \n",
    "            # Delete all indexes:\n",
    "            result = session.run(\"SHOW INDEXES\")\n",
    "            for record in result:\n",
    "                index_name = record.get(\"name\") or record.get(\"indexName\")\n",
    "                if index_name:\n",
    "                    try:\n",
    "                        session.run(f\"DROP INDEX {index_name}\")\n",
    "                        logger.info(f\"Dropped index: {index_name}\")\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Could not drop index {index_name}: {e}\")\n",
    "            \n",
    "            # Delete all constraints:\n",
    "            result = session.run(\"SHOW CONSTRAINTS\")\n",
    "            for record in result:\n",
    "                constraint_name = record.get(\"name\")\n",
    "                if constraint_name:\n",
    "                    try:\n",
    "                        session.run(f\"DROP CONSTRAINT {constraint_name}\")\n",
    "                        logger.info(f\"Dropped constraint: {constraint_name}\")\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Could not drop constraint {constraint_name}: {e}\")\n",
    "            \n",
    "            logger.info(\"Database completely cleared\")\n",
    "\n",
    "    def create_constraints(self):\n",
    "        constraints = [\n",
    "            \"CREATE CONSTRAINT activity_id IF NOT EXISTS FOR (a:Activity) REQUIRE a.activityId IS UNIQUE\",\n",
    "            \"CREATE CONSTRAINT asset_id IF NOT EXISTS FOR (a:Asset) REQUIRE a.assetId IS UNIQUE\",\n",
    "            \"CREATE CONSTRAINT location_id IF NOT EXISTS FOR (l:Location) REQUIRE l.locationId IS UNIQUE\",\n",
    "            \"CREATE CONSTRAINT request_id IF NOT EXISTS FOR (s:ServiceRequest) REQUIRE s.requestId IS UNIQUE\"\n",
    "        ]\n",
    "        \n",
    "        with self.driver.session() as session:\n",
    "            for constraint in constraints:\n",
    "                try:\n",
    "                    session.run(constraint)\n",
    "                    logger.info(f\"Created constraint: {constraint.split('FOR')[1].split('REQUIRE')[0].strip()}\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Constraint may already exist: {e}\")\n",
    "\n",
    "\n",
    "    def create_indexes(self):\n",
    "        \n",
    "        indexes = [\n",
    "            # \"CREATE INDEX IF NOT EXISTS FOR (a:Asset) ON (a.assetId)\",\n",
    "\n",
    "            \"CREATE INDEX IF NOT EXISTS FOR (s:ServiceRequest) ON (s.createdYear)\",\n",
    "            \"CREATE INDEX IF NOT EXISTS FOR (s:ServiceRequest) ON (s.isCompleted)\",\n",
    "            \"CREATE INDEX IF NOT EXISTS FOR (s:ServiceRequest) ON (s.createdMonth)\",\n",
    "            \"CREATE INDEX IF NOT EXISTS FOR (s:ServiceRequest) ON (s.sla)\",\n",
    "            # \"CREATE INDEX IF NOT EXISTS FOR (s:ServiceRequest) ON (s.requestId)\",\n",
    "            \"CREATE INDEX IF NOT EXISTS FOR (s:ServiceRequest) ON (s.serviceClassificationId)\" ,\n",
    "\n",
    "            \"CREATE INDEX IF NOT EXISTS FOR (c:Customer) ON (c.customer)\",\n",
    "            \n",
    "            # \"CREATE INDEX IF NOT EXISTS FOR (l:Location) ON (l.locationId),\"\n",
    "\n",
    "            # \"CREATE INDEX IF NOT EXISTS FOR (ac:Activity) ON (ac.activityId),\"\n",
    "\n",
    "            \"CREATE INDEX IF NOT EXISTS FOR (cn:Country) ON (cn.country)\"\n",
    "\n",
    "        ]\n",
    "        \n",
    "        with self.driver.session() as session:\n",
    "            for index in indexes:\n",
    "                try:\n",
    "                    session.run(index)\n",
    "                    logger.info(f\"Created index: {index}\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Index may already exist: {e}\")\n",
    "\n",
    "\n",
    "    def load_nodes_from_csv(self, csv, node_label: str, id_property: str, batch_size: int = 1000):\n",
    "        \"\"\"Load nodes from CSV file in batches.\"\"\"\n",
    "        df = csv\n",
    "        df = df.where(pd.notnull(df), None)  # Replace NaN with None\n",
    "        \n",
    "        total_rows = len(df)\n",
    "        # logger.info(f\"Loading {total_rows} {node_label} nodes from {csv_path}\")\n",
    "        \n",
    "        with self.driver.session() as session:\n",
    "            for i in range(0, total_rows, batch_size):\n",
    "                batch = df.iloc[i:i+batch_size]\n",
    "                records = batch.to_dict('records')\n",
    "                \n",
    "                # Build Cypher query dynamically\n",
    "                query = f\"\"\"\n",
    "                UNWIND $records AS record\n",
    "                MERGE (n:{node_label} {{{id_property}: record.{id_property}}})\n",
    "                SET n += record\n",
    "                \"\"\"\n",
    "                \n",
    "                session.run(query, records=records)\n",
    "                logger.info(f\"Loaded batch {i//batch_size + 1}/{(total_rows-1)//batch_size + 1} for {node_label}\")\n",
    "        \n",
    "        logger.info(f\"Completed loading {node_label} nodes\")\n",
    "\n",
    "\n",
    "    def load_relationships_from_csv(self, csv, rel_config: Dict, batch_size: int = 1000):\n",
    "        \"\"\"\n",
    "        Load relationships from CSV file.\n",
    "        \n",
    "        rel_config example:\n",
    "        {\n",
    "            'rel_type': 'LOCATED_AT',\n",
    "            'from_label': 'Asset',\n",
    "            'from_id_col': 'assetId',\n",
    "            'from_id_prop': 'assetId',\n",
    "            'to_label': 'Location',\n",
    "            'to_id_col': 'locationId',\n",
    "            'to_id_prop': 'locationId',\n",
    "            'properties': []  # Optional: list of relationship properties\n",
    "        }\n",
    "        \"\"\"\n",
    "        df = csv\n",
    "        df = df.where(pd.notnull(df), None)\n",
    "        \n",
    "        total_rows = len(df)\n",
    "        # logger.info(f\"Loading {total_rows} {rel_config['rel_type']} relationships from {csv_path}\")\n",
    "        \n",
    "        with self.driver.session() as session:\n",
    "            for i in range(0, total_rows, batch_size):\n",
    "                batch = df.iloc[i:i+batch_size]\n",
    "                records = batch.to_dict('records')\n",
    "                \n",
    "                # Build relationship properties string if any\n",
    "                rel_props = \"\"\n",
    "                if rel_config.get('properties'):\n",
    "                    props_str = \", \".join([f\"{p}: record.{p}\" for p in rel_config['properties']])\n",
    "                    rel_props = f\" {{{props_str}}}\"\n",
    "                \n",
    "                query = f\"\"\"\n",
    "                UNWIND $records AS record\n",
    "                MATCH (from:{rel_config['from_label']} {{{rel_config['from_id_prop']}: record.{rel_config['from_id_col']}}})\n",
    "                MATCH (to:{rel_config['to_label']} {{{rel_config['to_id_prop']}: record.{rel_config['to_id_col']}}})\n",
    "                MERGE (from)-[r:{rel_config['rel_type']}]->(to)\n",
    "                \"\"\"\n",
    "                \n",
    "                if rel_props:\n",
    "                    query += f\"\\nSET r += {{{', '.join([f'{p}: record.{p}' for p in rel_config['properties']])}}}\"\n",
    "                \n",
    "                session.run(query, records=records)\n",
    "                logger.info(f\"Loaded batch {i//batch_size + 1}/{(total_rows-1)//batch_size + 1} for {rel_config['rel_type']}\")\n",
    "        \n",
    "        logger.info(f\"Completed loading {rel_config['rel_type']} relationships\")\n",
    "    \n",
    "\n",
    "    def verify_load(self):\n",
    "        \"\"\"Verify the data load by counting nodes and relationships.\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            # Count nodes\n",
    "            node_labels = ['Activity', 'Asset', 'Country', 'Customer', 'Location', 'ServiceRequest']\n",
    "            for label in node_labels:\n",
    "                result = session.run(f\"MATCH (n:{label}) RETURN count(n) as count\")\n",
    "                count = result.single()['count']\n",
    "                logger.info(f\"{label} nodes: {count}\")\n",
    "            \n",
    "            # Count relationships\n",
    "            result = session.run(\"MATCH ()-[r]->() RETURN type(r) as type, count(r) as count\")\n",
    "            for record in result:\n",
    "                logger.info(f\"{record['type']} relationships: {record['count']}\")\n",
    "\n",
    "    \n",
    "    def migration_executor(self):\n",
    "\n",
    "        # # CAUTION: Deleting the existing graph!!!\n",
    "        # logger.info(\"=\" * 50)\n",
    "        # logger.info(\"DELETING THE EXISTING GRAPH!!\")\n",
    "        # logger.info(\"=\" * 50) \n",
    "        # self.clear_database()\n",
    "\n",
    "        logger.info(\"=\" * 50)\n",
    "        logger.info(\"LOADING NODES\")\n",
    "        logger.info(\"=\" * 50)        \n",
    "        self.load_nodes()\n",
    "\n",
    "        logger.info(\"=\" * 50)\n",
    "        logger.info(\"LOADING RELATIONSHIPS\")\n",
    "        logger.info(\"=\" * 50)\n",
    "        self.load_relationships()\n",
    "\n",
    "        logger.info(\"=\" * 50)\n",
    "        logger.info(\"VERIFYING NODE AND RELATIONSHP CREATION.\")\n",
    "        logger.info(\"=\" * 50)\n",
    "        self.verify_load()\n",
    "\n",
    "        logger.info(\"=\" * 50)\n",
    "        logger.info(\"DATA MIGRATION SUCCESSFUL!\")\n",
    "        logger.info(\"=\" * 50)\n",
    "\n",
    "        self.close()\n",
    "\n",
    "    \n",
    "    def load_nodes(self):\n",
    "        \n",
    "        # DATA_DIR = Path(\"./neo4j_data\")\n",
    "\n",
    "        try:\n",
    "\n",
    "            # # Create constraints and indexes              # not required when pushing data incrementally\n",
    "            # self.create_constraints()\n",
    "            # self.create_indexes()\n",
    "            \n",
    "            # Load nodes\n",
    "            \n",
    "            self.load_nodes_from_csv(\n",
    "                self.activity_df, \n",
    "                \"Activity\", \n",
    "                \"activityId\"\n",
    "            )\n",
    "            \n",
    "            self.load_nodes_from_csv(\n",
    "                self.assets_df, \n",
    "                \"Asset\", \n",
    "                \"assetId\"\n",
    "            )\n",
    "            \n",
    "            self.load_nodes_from_csv(\n",
    "                self.country_df, \n",
    "                \"Country\", \n",
    "                \"country\"\n",
    "            )\n",
    "            \n",
    "            self.load_nodes_from_csv(\n",
    "                self.customer_df, \n",
    "                \"Customer\", \n",
    "                \"customer\"\n",
    "            )\n",
    "            \n",
    "            self.load_nodes_from_csv(\n",
    "                self.location_df, \n",
    "                \"Location\", \n",
    "                \"locationId\"\n",
    "            )\n",
    "            \n",
    "            self.load_nodes_from_csv(\n",
    "                self.service_req_df, \n",
    "                \"ServiceRequest\", \n",
    "                \"requestId\"\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error creating Nodes in Neo4j: {e}\")\n",
    "\n",
    "\n",
    "    def load_relationships(self):\n",
    "        \n",
    "        # REL_DIR = Path(\"./neo4j_relationships\")\n",
    "\n",
    "        try:\n",
    "            # Load relationships\n",
    "            \n",
    "            # Asset -> Location\n",
    "            self.load_relationships_from_csv(\n",
    "                self.LOCATED_AT,\n",
    "                {\n",
    "                    'rel_type': 'LOCATED_AT',\n",
    "                    'from_label': 'Asset',\n",
    "                    'from_id_col': 'assetId',\n",
    "                    'from_id_prop': 'assetId',\n",
    "                    'to_label': 'Location',\n",
    "                    'to_id_col': 'locationId',\n",
    "                    'to_id_prop': 'locationId'\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # ServiceRequest -> Location\n",
    "            self.load_relationships_from_csv(\n",
    "                self.AT_LOCATION,\n",
    "                {\n",
    "                    'rel_type': 'AT_LOCATION',\n",
    "                    'from_label': 'ServiceRequest',\n",
    "                    'from_id_col': 'requestId',\n",
    "                    'from_id_prop': 'requestId',\n",
    "                    'to_label': 'Location',\n",
    "                    'to_id_col': 'locationId',\n",
    "                    'to_id_prop': 'locationId'\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # ServiceRequest -> Activity\n",
    "            self.load_relationships_from_csv(\n",
    "                self.HAS_ACTIVITY,\n",
    "                {\n",
    "                    'rel_type': 'HAS_ACTIVITY',\n",
    "                    'from_label': 'ServiceRequest',\n",
    "                    'from_id_col': 'requestId',\n",
    "                    'from_id_prop': 'requestId',\n",
    "                    'to_label': 'Activity',\n",
    "                    'to_id_col': 'activityId',\n",
    "                    'to_id_prop': 'activityId'\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # ServiceRequest -> Asset\n",
    "            self.load_relationships_from_csv(\n",
    "                self.FOR_ASSET,\n",
    "                {\n",
    "                    'rel_type': 'FOR_ASSET',\n",
    "                    'from_label': 'ServiceRequest',\n",
    "                    'from_id_col': 'requestId',\n",
    "                    'from_id_prop': 'requestId',\n",
    "                    'to_label': 'Asset',\n",
    "                    'to_id_col': 'assetId',\n",
    "                    'to_id_prop': 'assetId'\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # Customer -> Country\n",
    "            self.load_relationships_from_csv(\n",
    "                self.OPERATES_IN,\n",
    "                {\n",
    "                    'rel_type': 'OPERATES_IN',\n",
    "                    'from_label': 'Customer',\n",
    "                    'from_id_col': 'customer',\n",
    "                    'from_id_prop': 'customer',\n",
    "                    'to_label': 'Country',\n",
    "                    'to_id_col': 'country',\n",
    "                    'to_id_prop': 'country'\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # Customer -> Location\n",
    "            self.load_relationships_from_csv(\n",
    "                self.RESIDES_AT,\n",
    "                {\n",
    "                    'rel_type': 'RESIDES_AT',\n",
    "                    'from_label': 'Customer',\n",
    "                    'from_id_col': 'customer',\n",
    "                    'from_id_prop': 'customer',\n",
    "                    'to_label': 'Location',\n",
    "                    'to_id_col': 'locationId',\n",
    "                    'to_id_prop': 'locationId'\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # Customer -> Asset\n",
    "            self.load_relationships_from_csv(\n",
    "                self.OWNS,\n",
    "                {\n",
    "                    'rel_type': 'OWNS',\n",
    "                    'from_label': 'Customer',\n",
    "                    'from_id_col': 'customer',\n",
    "                    'from_id_prop': 'customer',\n",
    "                    'to_label': 'Asset',\n",
    "                    'to_id_col': 'assetId',\n",
    "                    'to_id_prop': 'assetId'\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # Customer -> ServiceRequest\n",
    "            self.load_relationships_from_csv(\n",
    "                self.CREATES,\n",
    "                {\n",
    "                    'rel_type': 'CREATES',\n",
    "                    'from_label': 'Customer',\n",
    "                    'from_id_col': 'customer',\n",
    "                    'from_id_prop': 'customer',\n",
    "                    'to_label': 'ServiceRequest',\n",
    "                    'to_id_col': 'requestId',\n",
    "                    'to_id_prop': 'requestId'\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # Location -> Country\n",
    "            self.load_relationships_from_csv(\n",
    "                self.IN,\n",
    "                {\n",
    "                    'rel_type': 'IN',\n",
    "                    'from_label': 'Location',\n",
    "                    'from_id_col': 'locationId',\n",
    "                    'from_id_prop': 'locationId',\n",
    "                    'to_label': 'Country',\n",
    "                    'to_id_col': 'country',\n",
    "                    'to_id_prop': 'country'\n",
    "                }\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error creating Relationships for nodes on Neo4j: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ebf573",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Data last pushed on: 2025-10-27 00:00:00.001\n",
      "INFO:__main__:==================================================\n",
      "INFO:__main__:LOADING DATA\n",
      "INFO:__main__:==================================================\n",
      "INFO:__main__: Downloaded 440 rows from 'v_requests', 508838 rows from 'v_assets', and 443 rows from 'v_request_with_activities'.\n",
      "INFO:__main__:Helper CSV files loaded successfully.\n",
      "INFO:__main__:Node and Property data prepared!\n",
      "INFO:__main__:Relationships created!\n",
      "INFO:__main__:==================================================\n",
      "INFO:__main__:DATA LOADING SUCCESSFUL!\n",
      "INFO:__main__:==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of asset df after trimming: 47\n",
      "Length of final assets_df df: 47\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    dataMigrator = DataLoaderAndMigrator('config.ini')\n",
    "    dataMigrator.migration_executor()\n",
    "    \n",
    "except NameError:\n",
    "    logger.warning(\"The DataMigrator class is not defined. Please ensure it is defined correctly.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5662583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0503bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DataMigrator:\n",
    "\n",
    "#     def __init__(self, config_file_path):\n",
    "\n",
    "#         if not Path(config_file_path).exists():\n",
    "#             logger.warning(f\"Config file {config_file_path} not found!\")\n",
    "        \n",
    "#         config = configparser.ConfigParser()\n",
    "#         config.read(config_file_path)\n",
    "\n",
    "#         if 'Neo4j' not in config:\n",
    "#             raise ValueError(\"Neo4j section not found in config\")\n",
    "        \n",
    "#         nj_config = {\n",
    "#             'url': config['Neo4j']['url'],\n",
    "#             'username': config['Neo4j']['username'],\n",
    "#             'password': config['Neo4j']['password']\n",
    "#         }\n",
    "\n",
    "#         self.nj_url = nj_config.get('url')\n",
    "#         self.nj_username = nj_config.get('username')\n",
    "#         self.nj_password = nj_config.get('password')\n",
    "\n",
    "#         self.driver = GraphDatabase.driver(self.nj_url, auth=(self.nj_username, self.nj_password))\n",
    "        \n",
    "#         # self.migration_executor()\n",
    "\n",
    "#     def close(self):\n",
    "#         self.driver.close()\n",
    "    \n",
    "#     def clear_database(self):\n",
    "\n",
    "#         with self.driver.session() as session:\n",
    "\n",
    "#             # Delete all nodes and relationships:\n",
    "#             session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "#             logger.info(\"All nodes and relationships deleted\")\n",
    "            \n",
    "#             # Delete all indexes:\n",
    "#             result = session.run(\"SHOW INDEXES\")\n",
    "#             for record in result:\n",
    "#                 index_name = record.get(\"name\") or record.get(\"indexName\")\n",
    "#                 if index_name:\n",
    "#                     try:\n",
    "#                         session.run(f\"DROP INDEX {index_name}\")\n",
    "#                         logger.info(f\"Dropped index: {index_name}\")\n",
    "#                     except Exception as e:\n",
    "#                         logger.warning(f\"Could not drop index {index_name}: {e}\")\n",
    "            \n",
    "#             # Delete all constraints:\n",
    "#             result = session.run(\"SHOW CONSTRAINTS\")\n",
    "#             for record in result:\n",
    "#                 constraint_name = record.get(\"name\")\n",
    "#                 if constraint_name:\n",
    "#                     try:\n",
    "#                         session.run(f\"DROP CONSTRAINT {constraint_name}\")\n",
    "#                         logger.info(f\"Dropped constraint: {constraint_name}\")\n",
    "#                     except Exception as e:\n",
    "#                         logger.warning(f\"Could not drop constraint {constraint_name}: {e}\")\n",
    "            \n",
    "#             logger.info(\"Database completely cleared\")\n",
    "\n",
    "#     def create_constraints(self):\n",
    "#         constraints = [\n",
    "#             \"CREATE CONSTRAINT activity_id IF NOT EXISTS FOR (a:Activity) REQUIRE a.activityId IS UNIQUE\",\n",
    "#             \"CREATE CONSTRAINT asset_id IF NOT EXISTS FOR (a:Asset) REQUIRE a.assetId IS UNIQUE\",\n",
    "#             \"CREATE CONSTRAINT location_id IF NOT EXISTS FOR (l:Location) REQUIRE l.locationId IS UNIQUE\",\n",
    "#             \"CREATE CONSTRAINT request_id IF NOT EXISTS FOR (s:ServiceRequest) REQUIRE s.requestId IS UNIQUE\"\n",
    "#         ]\n",
    "        \n",
    "#         with self.driver.session() as session:\n",
    "#             for constraint in constraints:\n",
    "#                 try:\n",
    "#                     session.run(constraint)\n",
    "#                     logger.info(f\"Created constraint: {constraint.split('FOR')[1].split('REQUIRE')[0].strip()}\")\n",
    "#                 except Exception as e:\n",
    "#                     logger.warning(f\"Constraint may already exist: {e}\")\n",
    "\n",
    "\n",
    "#     def create_indexes(self):\n",
    "        \n",
    "#         indexes = [\n",
    "#             # \"CREATE INDEX IF NOT EXISTS FOR (a:Asset) ON (a.assetId)\",\n",
    "\n",
    "#             \"CREATE INDEX IF NOT EXISTS FOR (s:ServiceRequest) ON (s.createdYear)\",\n",
    "#             \"CREATE INDEX IF NOT EXISTS FOR (s:ServiceRequest) ON (s.isCompleted)\",\n",
    "#             \"CREATE INDEX IF NOT EXISTS FOR (s:ServiceRequest) ON (s.createdMonth)\",\n",
    "#             \"CREATE INDEX IF NOT EXISTS FOR (s:ServiceRequest) ON (s.sla)\",\n",
    "#             # \"CREATE INDEX IF NOT EXISTS FOR (s:ServiceRequest) ON (s.requestId)\",\n",
    "#             \"CREATE INDEX IF NOT EXISTS FOR (s:ServiceRequest) ON (s.serviceClassificationId)\" ,\n",
    "\n",
    "#             \"CREATE INDEX IF NOT EXISTS FOR (c:Customer) ON (c.customer)\",\n",
    "            \n",
    "#             # \"CREATE INDEX IF NOT EXISTS FOR (l:Location) ON (l.locationId),\"\n",
    "\n",
    "#             # \"CREATE INDEX IF NOT EXISTS FOR (ac:Activity) ON (ac.activityId),\"\n",
    "\n",
    "#             \"CREATE INDEX IF NOT EXISTS FOR (cn:Country) ON (cn.country)\"\n",
    "\n",
    "#         ]\n",
    "        \n",
    "#         with self.driver.session() as session:\n",
    "#             for index in indexes:\n",
    "#                 try:\n",
    "#                     session.run(index)\n",
    "#                     logger.info(f\"Created index: {index}\")\n",
    "#                 except Exception as e:\n",
    "#                     logger.warning(f\"Index may already exist: {e}\")\n",
    "\n",
    "\n",
    "#     def load_nodes_from_csv(self, csv, node_label: str, id_property: str, batch_size: int = 1000):\n",
    "#         \"\"\"Load nodes from CSV file in batches.\"\"\"\n",
    "#         df = csv\n",
    "#         df = df.where(pd.notnull(df), None)  # Replace NaN with None\n",
    "        \n",
    "#         total_rows = len(df)\n",
    "#         # logger.info(f\"Loading {total_rows} {node_label} nodes from {csv_path}\")\n",
    "        \n",
    "#         with self.driver.session() as session:\n",
    "#             for i in range(0, total_rows, batch_size):\n",
    "#                 batch = df.iloc[i:i+batch_size]\n",
    "#                 records = batch.to_dict('records')\n",
    "                \n",
    "#                 # Build Cypher query dynamically\n",
    "#                 query = f\"\"\"\n",
    "#                 UNWIND $records AS record\n",
    "#                 MERGE (n:{node_label} {{{id_property}: record.{id_property}}})\n",
    "#                 SET n += record\n",
    "#                 \"\"\"\n",
    "                \n",
    "#                 session.run(query, records=records)\n",
    "#                 logger.info(f\"Loaded batch {i//batch_size + 1}/{(total_rows-1)//batch_size + 1} for {node_label}\")\n",
    "        \n",
    "#         logger.info(f\"Completed loading {node_label} nodes\")\n",
    "\n",
    "\n",
    "#     def load_relationships_from_csv(self, csv, rel_config: Dict, batch_size: int = 1000):\n",
    "#         \"\"\"\n",
    "#         Load relationships from CSV file.\n",
    "        \n",
    "#         rel_config example:\n",
    "#         {\n",
    "#             'rel_type': 'LOCATED_AT',\n",
    "#             'from_label': 'Asset',\n",
    "#             'from_id_col': 'assetId',\n",
    "#             'from_id_prop': 'assetId',\n",
    "#             'to_label': 'Location',\n",
    "#             'to_id_col': 'locationId',\n",
    "#             'to_id_prop': 'locationId',\n",
    "#             'properties': []  # Optional: list of relationship properties\n",
    "#         }\n",
    "#         \"\"\"\n",
    "#         df = csv\n",
    "#         df = df.where(pd.notnull(df), None)\n",
    "        \n",
    "#         total_rows = len(df)\n",
    "#         # logger.info(f\"Loading {total_rows} {rel_config['rel_type']} relationships from {csv_path}\")\n",
    "        \n",
    "#         with self.driver.session() as session:\n",
    "#             for i in range(0, total_rows, batch_size):\n",
    "#                 batch = df.iloc[i:i+batch_size]\n",
    "#                 records = batch.to_dict('records')\n",
    "                \n",
    "#                 # Build relationship properties string if any\n",
    "#                 rel_props = \"\"\n",
    "#                 if rel_config.get('properties'):\n",
    "#                     props_str = \", \".join([f\"{p}: record.{p}\" for p in rel_config['properties']])\n",
    "#                     rel_props = f\" {{{props_str}}}\"\n",
    "                \n",
    "#                 query = f\"\"\"\n",
    "#                 UNWIND $records AS record\n",
    "#                 MATCH (from:{rel_config['from_label']} {{{rel_config['from_id_prop']}: record.{rel_config['from_id_col']}}})\n",
    "#                 MATCH (to:{rel_config['to_label']} {{{rel_config['to_id_prop']}: record.{rel_config['to_id_col']}}})\n",
    "#                 MERGE (from)-[r:{rel_config['rel_type']}]->(to)\n",
    "#                 \"\"\"\n",
    "                \n",
    "#                 if rel_props:\n",
    "#                     query += f\"\\nSET r += {{{', '.join([f'{p}: record.{p}' for p in rel_config['properties']])}}}\"\n",
    "                \n",
    "#                 session.run(query, records=records)\n",
    "#                 logger.info(f\"Loaded batch {i//batch_size + 1}/{(total_rows-1)//batch_size + 1} for {rel_config['rel_type']}\")\n",
    "        \n",
    "#         logger.info(f\"Completed loading {rel_config['rel_type']} relationships\")\n",
    "    \n",
    "\n",
    "#     def verify_load(self):\n",
    "#         \"\"\"Verify the data load by counting nodes and relationships.\"\"\"\n",
    "#         with self.driver.session() as session:\n",
    "#             # Count nodes\n",
    "#             node_labels = ['Activity', 'Asset', 'Country', 'Customer', 'Location', 'ServiceRequest']\n",
    "#             for label in node_labels:\n",
    "#                 result = session.run(f\"MATCH (n:{label}) RETURN count(n) as count\")\n",
    "#                 count = result.single()['count']\n",
    "#                 logger.info(f\"{label} nodes: {count}\")\n",
    "            \n",
    "#             # Count relationships\n",
    "#             result = session.run(\"MATCH ()-[r]->() RETURN type(r) as type, count(r) as count\")\n",
    "#             for record in result:\n",
    "#                 logger.info(f\"{record['type']} relationships: {record['count']}\")\n",
    "\n",
    "    \n",
    "#     def migration_executor(self):\n",
    "\n",
    "#         # # CAUTION: Deleting the existing graph!!!\n",
    "#         # logger.info(\"=\" * 50)\n",
    "#         # logger.info(\"DELETING THE EXISTING GRAPH!!\")\n",
    "#         # logger.info(\"=\" * 50) \n",
    "#         # self.clear_database()\n",
    "\n",
    "#         logger.info(\"=\" * 50)\n",
    "#         logger.info(\"LOADING NODES\")\n",
    "#         logger.info(\"=\" * 50)        \n",
    "#         self.load_nodes()\n",
    "\n",
    "#         logger.info(\"=\" * 50)\n",
    "#         logger.info(\"LOADING RELATIONSHIPS\")\n",
    "#         logger.info(\"=\" * 50)\n",
    "#         self.load_relationships()\n",
    "\n",
    "#         logger.info(\"=\" * 50)\n",
    "#         logger.info(\"VERIFYING NODE AND RELATIONSHP CREATION.\")\n",
    "#         logger.info(\"=\" * 50)\n",
    "#         self.verify_load()\n",
    "\n",
    "#         logger.info(\"=\" * 50)\n",
    "#         logger.info(\"DATA MIGRATION SUCCESSFUL!\")\n",
    "#         logger.info(\"=\" * 50)\n",
    "\n",
    "#         self.close()\n",
    "\n",
    "    \n",
    "#     def load_nodes(self):\n",
    "        \n",
    "#         # DATA_DIR = Path(\"./neo4j_data\")\n",
    "\n",
    "#         try:\n",
    "\n",
    "#             # # Create constraints and indexes              # not required when pushing data incrementally\n",
    "#             # self.create_constraints()\n",
    "#             # self.create_indexes()\n",
    "            \n",
    "#             # Load nodes\n",
    "            \n",
    "#             self.load_nodes_from_csv(\n",
    "#                 self.activity_df, \n",
    "#                 \"Activity\", \n",
    "#                 \"activityId\"\n",
    "#             )\n",
    "            \n",
    "#             self.load_nodes_from_csv(\n",
    "#                 self.assets_df, \n",
    "#                 \"Asset\", \n",
    "#                 \"assetId\"\n",
    "#             )\n",
    "            \n",
    "#             self.load_nodes_from_csv(\n",
    "#                 self.country_df, \n",
    "#                 \"Country\", \n",
    "#                 \"country\"\n",
    "#             )\n",
    "            \n",
    "#             self.load_nodes_from_csv(\n",
    "#                 self.customer_df, \n",
    "#                 \"Customer\", \n",
    "#                 \"customer\"\n",
    "#             )\n",
    "            \n",
    "#             self.load_nodes_from_csv(\n",
    "#                 self.location_df, \n",
    "#                 \"Location\", \n",
    "#                 \"locationId\"\n",
    "#             )\n",
    "            \n",
    "#             self.load_nodes_from_csv(\n",
    "#                 self.service_req_df, \n",
    "#                 \"ServiceRequest\", \n",
    "#                 \"requestId\"\n",
    "#             )\n",
    "\n",
    "#         except Exception as e:\n",
    "#             logger.warning(f\"Error creating Nodes in Neo4j: {e}\")\n",
    "\n",
    "\n",
    "#     def load_relationships(self):\n",
    "        \n",
    "#         # REL_DIR = Path(\"./neo4j_relationships\")\n",
    "\n",
    "#         try:\n",
    "#             # Load relationships\n",
    "            \n",
    "#             # Asset -> Location\n",
    "#             self.load_relationships_from_csv(\n",
    "#                 self.LOCATED_AT,\n",
    "#                 {\n",
    "#                     'rel_type': 'LOCATED_AT',\n",
    "#                     'from_label': 'Asset',\n",
    "#                     'from_id_col': 'assetId',\n",
    "#                     'from_id_prop': 'assetId',\n",
    "#                     'to_label': 'Location',\n",
    "#                     'to_id_col': 'locationId',\n",
    "#                     'to_id_prop': 'locationId'\n",
    "#                 }\n",
    "#             )\n",
    "            \n",
    "#             # ServiceRequest -> Location\n",
    "#             self.load_relationships_from_csv(\n",
    "#                 self.AT_LOCATION,\n",
    "#                 {\n",
    "#                     'rel_type': 'AT_LOCATION',\n",
    "#                     'from_label': 'ServiceRequest',\n",
    "#                     'from_id_col': 'requestId',\n",
    "#                     'from_id_prop': 'requestId',\n",
    "#                     'to_label': 'Location',\n",
    "#                     'to_id_col': 'locationId',\n",
    "#                     'to_id_prop': 'locationId'\n",
    "#                 }\n",
    "#             )\n",
    "            \n",
    "#             # ServiceRequest -> Activity\n",
    "#             self.load_relationships_from_csv(\n",
    "#                 self.HAS_ACTIVITY,\n",
    "#                 {\n",
    "#                     'rel_type': 'HAS_ACTIVITY',\n",
    "#                     'from_label': 'ServiceRequest',\n",
    "#                     'from_id_col': 'requestId',\n",
    "#                     'from_id_prop': 'requestId',\n",
    "#                     'to_label': 'Activity',\n",
    "#                     'to_id_col': 'activityId',\n",
    "#                     'to_id_prop': 'activityId'\n",
    "#                 }\n",
    "#             )\n",
    "            \n",
    "#             # ServiceRequest -> Asset\n",
    "#             self.load_relationships_from_csv(\n",
    "#                 self.FOR_ASSET,\n",
    "#                 {\n",
    "#                     'rel_type': 'FOR_ASSET',\n",
    "#                     'from_label': 'ServiceRequest',\n",
    "#                     'from_id_col': 'requestId',\n",
    "#                     'from_id_prop': 'requestId',\n",
    "#                     'to_label': 'Asset',\n",
    "#                     'to_id_col': 'assetId',\n",
    "#                     'to_id_prop': 'assetId'\n",
    "#                 }\n",
    "#             )\n",
    "            \n",
    "#             # Customer -> Country\n",
    "#             self.load_relationships_from_csv(\n",
    "#                 self.OPERATES_IN,\n",
    "#                 {\n",
    "#                     'rel_type': 'OPERATES_IN',\n",
    "#                     'from_label': 'Customer',\n",
    "#                     'from_id_col': 'customer',\n",
    "#                     'from_id_prop': 'customer',\n",
    "#                     'to_label': 'Country',\n",
    "#                     'to_id_col': 'country',\n",
    "#                     'to_id_prop': 'country'\n",
    "#                 }\n",
    "#             )\n",
    "            \n",
    "#             # Customer -> Location\n",
    "#             self.load_relationships_from_csv(\n",
    "#                 self.RESIDES_AT,\n",
    "#                 {\n",
    "#                     'rel_type': 'RESIDES_AT',\n",
    "#                     'from_label': 'Customer',\n",
    "#                     'from_id_col': 'customer',\n",
    "#                     'from_id_prop': 'customer',\n",
    "#                     'to_label': 'Location',\n",
    "#                     'to_id_col': 'locationId',\n",
    "#                     'to_id_prop': 'locationId'\n",
    "#                 }\n",
    "#             )\n",
    "            \n",
    "#             # Customer -> Asset\n",
    "#             self.load_relationships_from_csv(\n",
    "#                 self.OWNS,\n",
    "#                 {\n",
    "#                     'rel_type': 'OWNS',\n",
    "#                     'from_label': 'Customer',\n",
    "#                     'from_id_col': 'customer',\n",
    "#                     'from_id_prop': 'customer',\n",
    "#                     'to_label': 'Asset',\n",
    "#                     'to_id_col': 'assetId',\n",
    "#                     'to_id_prop': 'assetId'\n",
    "#                 }\n",
    "#             )\n",
    "            \n",
    "#             # Customer -> ServiceRequest\n",
    "#             self.load_relationships_from_csv(\n",
    "#                 self.CREATES,\n",
    "#                 {\n",
    "#                     'rel_type': 'CREATES',\n",
    "#                     'from_label': 'Customer',\n",
    "#                     'from_id_col': 'customer',\n",
    "#                     'from_id_prop': 'customer',\n",
    "#                     'to_label': 'ServiceRequest',\n",
    "#                     'to_id_col': 'requestId',\n",
    "#                     'to_id_prop': 'requestId'\n",
    "#                 }\n",
    "#             )\n",
    "            \n",
    "#             # Location -> Country\n",
    "#             self.load_relationships_from_csv(\n",
    "#                 self.IN,\n",
    "#                 {\n",
    "#                     'rel_type': 'IN',\n",
    "#                     'from_label': 'Location',\n",
    "#                     'from_id_col': 'locationId',\n",
    "#                     'from_id_prop': 'locationId',\n",
    "#                     'to_label': 'Country',\n",
    "#                     'to_id_col': 'country',\n",
    "#                     'to_id_prop': 'country'\n",
    "#                 }\n",
    "#             )\n",
    "\n",
    "#         except Exception as e:\n",
    "#             logger.warning(f\"Error creating Relationships for nodes on Neo4j: {e}\")\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_injestion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
